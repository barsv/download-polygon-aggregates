{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "here claude sonnet 4 compared compression methods for saving and reading 20 years of 1 minute bars for SPY.\n",
    "based on test results, i select parquet with zstd compression.\n",
    "\n",
    "# Parquet Zstd (optimized) shows excellent balance:\n",
    "# - File size: 67.2MB (3rd smallest)\n",
    "# - Read speed: 0.071s (very fast)\n",
    "# - Write speed: 1.062s (reasonable)\n",
    "# - Best compression ratio vs speed trade-off\n",
    "\n",
    "# Comparison with other top performers:\n",
    "\n",
    "# Format          | Type      |    Write |     Read |      Size\n",
    "# -------------------------------------------------------------\n",
    "# Parquet None    | optimized |   1.013s |   0.048s |   101.7MB\n",
    "# Parquet Snappy  | optimized |   0.927s |   0.066s |    85.8MB\n",
    "# Parquet Gzip    | optimized |   4.971s |   0.096s |    61.5MB\n",
    "# Parquet Zstd    | optimized |   1.062s |   0.071s |    67.2MB\n",
    "# Parquet Brotli  | optimized |   9.540s |   0.114s |    57.4MB\n",
    "# Parquet LZ4     | optimized |   0.932s |   0.067s |    85.2MB\n",
    "# Feather         | optimized |   0.186s |   0.069s |    88.6MB\n",
    "\n",
    "# Recommended storage format:\n",
    "def save_optimized_bars(df, filepath):\n",
    "    \"\"\"Save DataFrame with optimized data types and compression\"\"\"\n",
    "    # Optimize data types\n",
    "    df_opt = df.copy()\n",
    "    df_opt['timestamp'] = pd.to_datetime(df_opt['timestamp']).astype('int64') // 10**9\n",
    "    df_opt['open'] = df_opt['open'].astype('float32')\n",
    "    df_opt['high'] = df_opt['high'].astype('float32')\n",
    "    df_opt['low'] = df_opt['low'].astype('float32')\n",
    "    df_opt['close'] = df_opt['close'].astype('float32')\n",
    "    df_opt['volume'] = df_opt['volume'].astype('uint32')\n",
    "    df_opt['vwap'] = df_opt['vwap'].astype('float32')\n",
    "    df_opt['transactions'] = df_opt['transactions'].astype('uint32')\n",
    "    \n",
    "    # Save with Zstd compression\n",
    "    df_opt.to_parquet(filepath, engine='pyarrow', compression='zstd')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83621d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow\n",
    "!pip install tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e346979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING WITH DATETIME TIMESTAMPS ===\n",
      "\n",
      "--- Writing Performance (datetime timestamps) ---\n",
      "CSV             | Write: 15.965s | Size: 260.2MB\n",
      "Parquet None    | Write: 0.759s | Size: 131.2MB\n",
      "Parquet Snappy  | Write: 0.870s | Size: 103.2MB\n",
      "Parquet Gzip    | Write: 6.708s | Size: 74.0MB\n",
      "Parquet Zstd    | Write: 0.960s | Size: 90.1MB\n",
      "Parquet Brotli  | Write: 10.537s | Size: 68.9MB\n",
      "Parquet LZ4     | Write: 0.865s | Size: 104.9MB\n",
      "Feather         | Write: 0.236s | Size: 125.1MB\n",
      "HDF5            | Write: 0.344s | Size: 302.3MB\n",
      "Numpy NPZ       | Write: 19.961s | Size: 91.3MB\n",
      "\n",
      "--- Reading Performance (datetime timestamps) ---\n",
      "CSV             | Read: 2.145s\n",
      "Parquet None    | Read: 0.119s\n",
      "Parquet Snappy  | Read: 0.112s\n",
      "Parquet Gzip    | Read: 0.112s\n",
      "Parquet Zstd    | Read: 0.085s\n",
      "Parquet Brotli  | Read: 0.151s\n",
      "Parquet LZ4     | Read: 0.087s\n",
      "Feather         | Read: 0.082s\n",
      "HDF5            | Read: 0.202s\n",
      "Numpy NPZ       | Read: 12.719s\n",
      "\n",
      "=== TESTING WITH STRING TIMESTAMPS ===\n",
      "\n",
      "--- Writing Performance (string timestamps) ---\n",
      "CSV             | Write: 13.816s | Size: 260.2MB\n",
      "Parquet None    | Write: 1.081s | Size: 187.2MB\n",
      "Parquet Snappy  | Write: 1.153s | Size: 92.2MB\n",
      "Parquet Gzip    | Write: 7.009s | Size: 65.8MB\n",
      "Parquet Zstd    | Write: 1.259s | Size: 68.5MB\n",
      "Parquet Brotli  | Write: 9.161s | Size: 56.6MB\n",
      "Parquet LZ4     | Write: 1.156s | Size: 93.7MB\n",
      "Feather         | Write: 0.474s | Size: 126.3MB\n",
      "HDF5            | Write: 1.231s | Size: 356.2MB\n",
      "Numpy NPZ       | Write: 11.312s | Size: 79.6MB\n",
      "\n",
      "--- Reading Performance (string timestamps) ---\n",
      "CSV             | Read: 1.916s\n",
      "Parquet None    | Read: 1.321s\n",
      "Parquet Snappy  | Read: 1.151s\n",
      "Parquet Gzip    | Read: 1.334s\n",
      "Parquet Zstd    | Read: 1.235s\n",
      "Parquet Brotli  | Read: 1.534s\n",
      "Parquet LZ4     | Read: 1.308s\n",
      "Feather         | Read: 1.483s\n",
      "HDF5            | Read: 0.829s\n",
      "Numpy NPZ       | Read: 3.161s\n",
      "\n",
      "=== TESTING WITH UNIX TIMESTAMPS ===\n",
      "\n",
      "--- Writing Performance (unix timestamps) ---\n",
      "CSV             | Write: 12.928s | Size: 226.1MB\n",
      "Parquet None    | Write: 0.839s | Size: 131.2MB\n",
      "Parquet Snappy  | Write: 0.943s | Size: 92.8MB\n",
      "Parquet Gzip    | Write: 6.648s | Size: 62.6MB\n",
      "Parquet Zstd    | Write: 1.131s | Size: 69.7MB\n",
      "Parquet Brotli  | Write: 9.900s | Size: 58.1MB\n",
      "Parquet LZ4     | Write: 1.007s | Size: 90.8MB\n",
      "Feather         | Write: 0.269s | Size: 111.0MB\n",
      "HDF5            | Write: 0.439s | Size: 302.3MB\n",
      "Numpy NPZ       | Write: 11.778s | Size: 62.3MB\n",
      "\n",
      "--- Reading Performance (unix timestamps) ---\n",
      "CSV             | Read: 1.295s\n",
      "Parquet None    | Read: 0.093s\n",
      "Parquet Snappy  | Read: 0.115s\n",
      "Parquet Gzip    | Read: 0.157s\n",
      "Parquet Zstd    | Read: 0.116s\n",
      "Parquet Brotli  | Read: 0.180s\n",
      "Parquet LZ4     | Read: 0.108s\n",
      "Feather         | Read: 0.119s\n",
      "HDF5            | Read: 0.247s\n",
      "Numpy NPZ       | Read: 0.661s\n",
      "\n",
      "=== TESTING WITH OPTIMIZED DATA TYPES ===\n",
      "Original data types:\n",
      "timestamp       datetime64[ns]\n",
      "open                   float64\n",
      "high                   float64\n",
      "low                    float64\n",
      "close                  float64\n",
      "volume                 float64\n",
      "vwap                   float64\n",
      "transactions             int64\n",
      "otc                    float64\n",
      "dtype: object\n",
      "\n",
      "Original memory usage: 272.0 MB\n",
      "\n",
      "Optimized data types:\n",
      "timestamp         int64\n",
      "open            float32\n",
      "high            float32\n",
      "low             float32\n",
      "close           float32\n",
      "volume           uint32\n",
      "vwap            float32\n",
      "transactions     uint32\n",
      "otc             float64\n",
      "dtype: object\n",
      "Optimized memory usage: 166.2 MB\n",
      "\n",
      "--- Writing Performance (optimized timestamps) ---\n",
      "CSV             | Write: 12.573s | Size: 218.6MB\n",
      "Parquet None    | Write: 1.013s | Size: 101.7MB\n",
      "Parquet Snappy  | Write: 0.927s | Size: 85.8MB\n",
      "Parquet Gzip    | Write: 4.971s | Size: 61.5MB\n",
      "Parquet Zstd    | Write: 1.062s | Size: 67.2MB\n",
      "Parquet Brotli  | Write: 9.540s | Size: 57.4MB\n",
      "Parquet LZ4     | Write: 0.932s | Size: 85.2MB\n",
      "Feather         | Write: 0.186s | Size: 88.6MB\n",
      "HDF5            | Write: 0.335s | Size: 196.5MB\n",
      "Numpy NPZ       | Write: 8.923s | Size: 60.9MB\n",
      "\n",
      "--- Reading Performance (optimized timestamps) ---\n",
      "CSV             | Read: 1.161s\n",
      "Parquet None    | Read: 0.048s\n",
      "Parquet Snappy  | Read: 0.066s\n",
      "Parquet Gzip    | Read: 0.096s\n",
      "Parquet Zstd    | Read: 0.071s\n",
      "Parquet Brotli  | Read: 0.114s\n",
      "Parquet LZ4     | Read: 0.067s\n",
      "Feather         | Read: 0.069s\n",
      "HDF5            | Read: 0.116s\n",
      "Numpy NPZ       | Read: 0.599s\n",
      "\n",
      "=== COMPREHENSIVE SUMMARY ===\n",
      "Format          | Type     |    Write |     Read |     Size\n",
      "------------------------------------------------------------\n",
      "CSV             | datetime |  15.965s |   2.145s |   260.2MB\n",
      "Parquet None    | datetime |   0.759s |   0.119s |   131.2MB\n",
      "Parquet Snappy  | datetime |   0.870s |   0.112s |   103.2MB\n",
      "Parquet Gzip    | datetime |   6.708s |   0.112s |    74.0MB\n",
      "Parquet Zstd    | datetime |   0.960s |   0.085s |    90.1MB\n",
      "Parquet Brotli  | datetime |  10.537s |   0.151s |    68.9MB\n",
      "Parquet LZ4     | datetime |   0.865s |   0.087s |   104.9MB\n",
      "Feather         | datetime |   0.236s |   0.082s |   125.1MB\n",
      "HDF5            | datetime |   0.344s |   0.202s |   302.3MB\n",
      "Numpy NPZ       | datetime |  19.961s |  12.719s |    91.3MB\n",
      "CSV             | string   |  13.816s |   1.916s |   260.2MB\n",
      "Parquet None    | string   |   1.081s |   1.321s |   187.2MB\n",
      "Parquet Snappy  | string   |   1.153s |   1.151s |    92.2MB\n",
      "Parquet Gzip    | string   |   7.009s |   1.334s |    65.8MB\n",
      "Parquet Zstd    | string   |   1.259s |   1.235s |    68.5MB\n",
      "Parquet Brotli  | string   |   9.161s |   1.534s |    56.6MB\n",
      "Parquet LZ4     | string   |   1.156s |   1.308s |    93.7MB\n",
      "Feather         | string   |   0.474s |   1.483s |   126.3MB\n",
      "HDF5            | string   |   1.231s |   0.829s |   356.2MB\n",
      "Numpy NPZ       | string   |  11.312s |   3.161s |    79.6MB\n",
      "CSV             | unix     |  12.928s |   1.295s |   226.1MB\n",
      "Parquet None    | unix     |   0.839s |   0.093s |   131.2MB\n",
      "Parquet Snappy  | unix     |   0.943s |   0.115s |    92.8MB\n",
      "Parquet Gzip    | unix     |   6.648s |   0.157s |    62.6MB\n",
      "Parquet Zstd    | unix     |   1.131s |   0.116s |    69.7MB\n",
      "Parquet Brotli  | unix     |   9.900s |   0.180s |    58.1MB\n",
      "Parquet LZ4     | unix     |   1.007s |   0.108s |    90.8MB\n",
      "Feather         | unix     |   0.269s |   0.119s |   111.0MB\n",
      "HDF5            | unix     |   0.439s |   0.247s |   302.3MB\n",
      "Numpy NPZ       | unix     |  11.778s |   0.661s |    62.3MB\n",
      "CSV             | optimized |  12.573s |   1.161s |   218.6MB\n",
      "Parquet None    | optimized |   1.013s |   0.048s |   101.7MB\n",
      "Parquet Snappy  | optimized |   0.927s |   0.066s |    85.8MB\n",
      "Parquet Gzip    | optimized |   4.971s |   0.096s |    61.5MB\n",
      "Parquet Zstd    | optimized |   1.062s |   0.071s |    67.2MB\n",
      "Parquet Brotli  | optimized |   9.540s |   0.114s |    57.4MB\n",
      "Parquet LZ4     | optimized |   0.932s |   0.067s |    85.2MB\n",
      "Feather         | optimized |   0.186s |   0.069s |    88.6MB\n",
      "HDF5            | optimized |   0.335s |   0.116s |   196.5MB\n",
      "Numpy NPZ       | optimized |   8.923s |   0.599s |    60.9MB\n",
      "\n",
      "=== BEST PERFORMERS ===\n",
      "Smallest file: Parquet Brotli (string) - 56.6MB\n",
      "Fastest read: Parquet None (optimized) - 0.048s\n",
      "Best balance: Parquet None (optimized) - 0.048s / 101.7MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to Python path to import api_key module\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "import settings\n",
    "\n",
    "def test_formats(df, dir_path, timestamp_type):\n",
    "    \"\"\"Test different file formats for saving and reading data\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Define test formats\n",
    "    formats = [\n",
    "        # CSV\n",
    "        ('CSV', lambda: df.to_csv(os.path.join(dir_path, f'SPY_{timestamp_type}.csv'), index=False)),\n",
    "        \n",
    "        # Parquet with different compressions\n",
    "        ('Parquet None', lambda: df.to_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_none.parquet'), \n",
    "                                               engine='pyarrow', compression=None)),\n",
    "        ('Parquet Snappy', lambda: df.to_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_snappy.parquet'), \n",
    "                                                  engine='pyarrow', compression='snappy')),\n",
    "        ('Parquet Gzip', lambda: df.to_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_gzip.parquet'), \n",
    "                                                engine='pyarrow', compression='gzip')),\n",
    "        ('Parquet Zstd', lambda: df.to_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_zstd.parquet'), \n",
    "                                                engine='pyarrow', compression='zstd')),\n",
    "        ('Parquet Brotli', lambda: df.to_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_brotli.parquet'), \n",
    "                                                  engine='pyarrow', compression='brotli')),\n",
    "        ('Parquet LZ4', lambda: df.to_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_lz4.parquet'), \n",
    "                                               engine='pyarrow', compression='lz4')),\n",
    "        \n",
    "        # Other formats\n",
    "        ('Feather', lambda: df.to_feather(os.path.join(dir_path, f'SPY_{timestamp_type}.feather'))),\n",
    "        ('HDF5', lambda: df.to_hdf(os.path.join(dir_path, f'SPY_{timestamp_type}.hdf'), key='SPY', mode='w')),\n",
    "        ('Numpy NPZ', lambda: np.savez_compressed(os.path.join(dir_path, f'SPY_{timestamp_type}.npz'), \n",
    "                                                   data=df.to_numpy(), columns=df.columns.to_numpy())),\n",
    "    ]\n",
    "    \n",
    "    # Test writing speed and file size\n",
    "    print(f\"\\n--- Writing Performance ({timestamp_type} timestamps) ---\")\n",
    "    for name, write_func in formats:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            write_func()\n",
    "            write_time = time.time() - start_time\n",
    "            \n",
    "            # Get file size\n",
    "            if 'CSV' in name:\n",
    "                file_path = os.path.join(dir_path, f'SPY_{timestamp_type}.csv')\n",
    "            elif 'Parquet' in name:\n",
    "                compression = name.split()[-1].lower()\n",
    "                file_path = os.path.join(dir_path, f'SPY_{timestamp_type}_{compression}.parquet')\n",
    "            elif 'Feather' in name:\n",
    "                file_path = os.path.join(dir_path, f'SPY_{timestamp_type}.feather')\n",
    "            elif 'HDF5' in name:\n",
    "                file_path = os.path.join(dir_path, f'SPY_{timestamp_type}.hdf')\n",
    "            elif 'Numpy' in name:\n",
    "                file_path = os.path.join(dir_path, f'SPY_{timestamp_type}.npz')\n",
    "            \n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "            \n",
    "            results.append({\n",
    "                'format': name,\n",
    "                'write_time': write_time,\n",
    "                'file_size': file_size,\n",
    "                'timestamp_type': timestamp_type\n",
    "            })\n",
    "            \n",
    "            print(f\"{name:15} | Write: {write_time:.3f}s | Size: {file_size:.1f}MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:15} | Error: {e}\")\n",
    "    \n",
    "    # Test reading speed\n",
    "    print(f\"\\n--- Reading Performance ({timestamp_type} timestamps) ---\")\n",
    "    read_formats = [\n",
    "        ('CSV', lambda: pd.read_csv(os.path.join(dir_path, f'SPY_{timestamp_type}.csv'))),\n",
    "        ('Parquet None', lambda: pd.read_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_none.parquet'))),\n",
    "        ('Parquet Snappy', lambda: pd.read_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_snappy.parquet'))),\n",
    "        ('Parquet Gzip', lambda: pd.read_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_gzip.parquet'))),\n",
    "        ('Parquet Zstd', lambda: pd.read_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_zstd.parquet'))),\n",
    "        ('Parquet Brotli', lambda: pd.read_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_brotli.parquet'))),\n",
    "        ('Parquet LZ4', lambda: pd.read_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_lz4.parquet'))),\n",
    "        ('Feather', lambda: pd.read_feather(os.path.join(dir_path, f'SPY_{timestamp_type}.feather'))),\n",
    "        ('HDF5', lambda: pd.read_hdf(os.path.join(dir_path, f'SPY_{timestamp_type}.hdf'), key='SPY')),\n",
    "        ('Numpy NPZ', lambda: load_numpy_to_df(os.path.join(dir_path, f'SPY_{timestamp_type}.npz'))),\n",
    "    ]\n",
    "    \n",
    "    for name, read_func in read_formats:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            df_loaded = read_func()\n",
    "            read_time = time.time() - start_time\n",
    "            \n",
    "            # Update results\n",
    "            for result in results:\n",
    "                if result['format'] == name:\n",
    "                    result['read_time'] = read_time\n",
    "                    break\n",
    "            \n",
    "            print(f\"{name:15} | Read: {read_time:.3f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:15} | Error: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def load_numpy_to_df(file_path):\n",
    "    \"\"\"Helper function to load numpy file back to DataFrame\"\"\"\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    return pd.DataFrame(data['data'], columns=data['columns'])\n",
    "\n",
    "# Main test execution\n",
    "dir_path = os.path.join(settings.ABSOLUTE_DATA_DIR, 'bars', '1minute')\n",
    "all_results = []\n",
    "\n",
    "# Test with datetime objects (parsed dates)\n",
    "print(\"=== TESTING WITH DATETIME TIMESTAMPS ===\")\n",
    "df_datetime = pd.read_csv(os.path.join(dir_path, 'SPY.csv'), parse_dates=['timestamp'])\n",
    "df_datetime['timestamp'] = pd.to_datetime(df_datetime['timestamp'])\n",
    "results_datetime = test_formats(df_datetime, dir_path, \"datetime\")\n",
    "all_results.extend(results_datetime)\n",
    "\n",
    "# Test with string timestamps (original)\n",
    "print(\"\\n=== TESTING WITH STRING TIMESTAMPS ===\")\n",
    "df_string = pd.read_csv(os.path.join(dir_path, 'SPY.csv'))  # Load without parsing dates\n",
    "results_string = test_formats(df_string, dir_path, \"string\")\n",
    "all_results.extend(results_string)\n",
    "\n",
    "# Test with unix timestamps (converted to int64)\n",
    "print(\"\\n=== TESTING WITH UNIX TIMESTAMPS ===\")\n",
    "df_unix = pd.read_csv(os.path.join(dir_path, 'SPY.csv'))\n",
    "df_unix['timestamp'] = pd.to_datetime(df_unix['timestamp']).astype('int64') // 10**9\n",
    "results_unix = test_formats(df_unix, dir_path, \"unix\")\n",
    "all_results.extend(results_unix)\n",
    "\n",
    "# Test with optimized data types for disk storage\n",
    "print(\"\\n=== TESTING WITH OPTIMIZED DATA TYPES ===\")\n",
    "df_optimized = pd.read_csv(os.path.join(dir_path, 'SPY.csv'))\n",
    "\n",
    "# Optimize data types for better compression and storage\n",
    "df_optimized['timestamp'] = pd.to_datetime(df_optimized['timestamp']).astype('int64') // 10**9  # Unix timestamp\n",
    "df_optimized['open'] = df_optimized['open'].astype('float32')      # 32-bit float instead of 64-bit\n",
    "df_optimized['high'] = df_optimized['high'].astype('float32')\n",
    "df_optimized['low'] = df_optimized['low'].astype('float32')\n",
    "df_optimized['close'] = df_optimized['close'].astype('float32')\n",
    "df_optimized['volume'] = df_optimized['volume'].astype('uint32')   # Unsigned 32-bit for volume\n",
    "df_optimized['vwap'] = df_optimized['vwap'].astype('float32')\n",
    "df_optimized['transactions'] = df_optimized['transactions'].astype('uint32')  # Unsigned 32-bit\n",
    "\n",
    "# Show data type info\n",
    "print(\"Original data types:\")\n",
    "print(df_datetime.dtypes)\n",
    "print(f\"\\nOriginal memory usage: {df_datetime.memory_usage(deep=True).sum() / (1024**2):.1f} MB\")\n",
    "\n",
    "print(\"\\nOptimized data types:\")\n",
    "print(df_optimized.dtypes)\n",
    "print(f\"Optimized memory usage: {df_optimized.memory_usage(deep=True).sum() / (1024**2):.1f} MB\")\n",
    "\n",
    "results_optimized = test_formats(df_optimized, dir_path, \"optimized\")\n",
    "all_results.extend(results_optimized)\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(\"\\n=== COMPREHENSIVE SUMMARY ===\")\n",
    "print(f\"{'Format':15} | {'Type':8} | {'Write':>8} | {'Read':>8} | {'Size':>8}\")\n",
    "print(\"-\" * 60)\n",
    "for result in all_results:\n",
    "    if 'read_time' in result:\n",
    "        print(f\"{result['format']:15} | {result['timestamp_type']:8} | \"\n",
    "              f\"{result['write_time']:7.3f}s | {result['read_time']:7.3f}s | \"\n",
    "              f\"{result['file_size']:7.1f}MB\")\n",
    "\n",
    "# Print best performers by category\n",
    "print(\"\\n=== BEST PERFORMERS ===\")\n",
    "# Best for file size\n",
    "best_size = min([r for r in all_results if 'read_time' in r], key=lambda x: x['file_size'])\n",
    "print(f\"Smallest file: {best_size['format']} ({best_size['timestamp_type']}) - {best_size['file_size']:.1f}MB\")\n",
    "\n",
    "# Best for read speed\n",
    "best_read = min([r for r in all_results if 'read_time' in r], key=lambda x: x['read_time'])\n",
    "print(f\"Fastest read: {best_read['format']} ({best_read['timestamp_type']}) - {best_read['read_time']:.3f}s\")\n",
    "\n",
    "# Best balance (read speed / file size ratio)\n",
    "for result in all_results:\n",
    "    if 'read_time' in result:\n",
    "        result['efficiency'] = result['read_time'] / result['file_size']\n",
    "best_balance = min([r for r in all_results if 'read_time' in r], key=lambda x: x['efficiency'])\n",
    "print(f\"Best balance: {best_balance['format']} ({best_balance['timestamp_type']}) - \"\n",
    "      f\"{best_balance['read_time']:.3f}s / {best_balance['file_size']:.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7a88659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA INTEGRITY TEST ===\n",
      "Original CSV shape: (3961817, 9)\n",
      "Original data types:\n",
      "timestamp        object\n",
      "open            float64\n",
      "high            float64\n",
      "low             float64\n",
      "close           float64\n",
      "volume          float64\n",
      "vwap            float64\n",
      "transactions      int64\n",
      "otc             float64\n",
      "dtype: object\n",
      "\n",
      "Optimized data types:\n",
      "timestamp         int64\n",
      "open            float32\n",
      "high            float32\n",
      "low             float32\n",
      "close           float32\n",
      "volume           uint32\n",
      "vwap            float32\n",
      "transactions     uint32\n",
      "otc             float64\n",
      "dtype: object\n",
      "Saved to: /home/stan/src/download-polygon-aggregates/../polygon-data/bars/1minute/SPY_integrity_test.parquet\n",
      "Loaded parquet shape: (3961817, 9)\n",
      "Loaded data types:\n",
      "timestamp         int64\n",
      "open            float32\n",
      "high            float32\n",
      "low             float32\n",
      "close           float32\n",
      "volume           uint32\n",
      "vwap            float32\n",
      "transactions     uint32\n",
      "otc             float64\n",
      "dtype: object\n",
      "✓ Shapes match\n",
      "✓ Column names match\n",
      "✓ Data types match\n",
      "\n",
      "Checking values row by row...\n",
      "Checking row 0 of 3,961,817\n",
      "Checking row 100,000 of 3,961,817\n",
      "Checking row 200,000 of 3,961,817\n",
      "Checking row 300,000 of 3,961,817\n",
      "Checking row 400,000 of 3,961,817\n",
      "Checking row 500,000 of 3,961,817\n",
      "Checking row 600,000 of 3,961,817\n",
      "Checking row 700,000 of 3,961,817\n",
      "Checking row 800,000 of 3,961,817\n",
      "Checking row 900,000 of 3,961,817\n",
      "Checking row 1,000,000 of 3,961,817\n",
      "Checking row 1,100,000 of 3,961,817\n",
      "Checking row 1,200,000 of 3,961,817\n",
      "Checking row 1,300,000 of 3,961,817\n",
      "Checking row 1,400,000 of 3,961,817\n",
      "Checking row 1,500,000 of 3,961,817\n",
      "Checking row 1,600,000 of 3,961,817\n",
      "Checking row 1,700,000 of 3,961,817\n",
      "Checking row 1,800,000 of 3,961,817\n",
      "Checking row 1,900,000 of 3,961,817\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChecking row \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_rows\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_optimized.columns:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     original_val = \u001b[43mdf_optimized\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m[col]\n\u001b[32m     68\u001b[39m     loaded_val = df_loaded.iloc[idx][col]\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# Handle NaN values\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/download-polygon-aggregates/venv/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/download-polygon-aggregates/venv/lib/python3.12/site-packages/pandas/core/indexing.py:1754\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1751\u001b[39m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[32m   1752\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_integer(key, axis)\n\u001b[32m-> \u001b[39m\u001b[32m1754\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/download-polygon-aggregates/venv/lib/python3.12/site-packages/pandas/core/frame.py:4001\u001b[39m, in \u001b[36mDataFrame._ixs\u001b[39m\u001b[34m(self, i, axis)\u001b[39m\n\u001b[32m   3999\u001b[39m \u001b[38;5;66;03m# irow\u001b[39;00m\n\u001b[32m   4000\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4001\u001b[39m     new_mgr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfast_xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4003\u001b[39m     \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[32m   4004\u001b[39m     copy = \u001b[38;5;28misinstance\u001b[39m(new_mgr.array, np.ndarray) \u001b[38;5;129;01mand\u001b[39;00m new_mgr.array.base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/download-polygon-aggregates/venv/lib/python3.12/site-packages/pandas/core/internals/managers.py:984\u001b[39m, in \u001b[36mBlockManager.fast_xs\u001b[39m\u001b[34m(self, loc)\u001b[39m\n\u001b[32m    976\u001b[39m     block = new_block(\n\u001b[32m    977\u001b[39m         result,\n\u001b[32m    978\u001b[39m         placement=bp,\n\u001b[32m    979\u001b[39m         ndim=\u001b[32m1\u001b[39m,\n\u001b[32m    980\u001b[39m         refs=\u001b[38;5;28mself\u001b[39m.blocks[\u001b[32m0\u001b[39m].refs,\n\u001b[32m    981\u001b[39m     )\n\u001b[32m    982\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SingleBlockManager(block, \u001b[38;5;28mself\u001b[39m.axes[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m984\u001b[39m dtype = interleaved_dtype([blk.dtype \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks])\n\u001b[32m    986\u001b[39m n = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[32m    989\u001b[39m     \u001b[38;5;66;03m# TODO: use object dtype as workaround for non-performant\u001b[39;00m\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m#  EA.__setitem__ methods. (primarily ArrowExtensionArray.__setitem__\u001b[39;00m\n\u001b[32m    991\u001b[39m     \u001b[38;5;66;03m#  when iteratively setting individual values)\u001b[39;00m\n\u001b[32m    992\u001b[39m     \u001b[38;5;66;03m#  https://github.com/pandas-dev/pandas/pull/54508#issuecomment-1675827918\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Data integrity test - verify that parquet saves and loads data correctly\n",
    "print(\"=== DATA INTEGRITY TEST ===\")\n",
    "\n",
    "# Read original CSV data\n",
    "df_original = pd.read_csv(os.path.join(dir_path, 'SPY.csv'))\n",
    "print(f\"Original CSV shape: {df_original.shape}\")\n",
    "print(f\"Original data types:\\n{df_original.dtypes}\")\n",
    "\n",
    "# Optimize data types (same as before)\n",
    "df_optimized = df_original.copy()\n",
    "df_optimized['timestamp'] = pd.to_datetime(df_optimized['timestamp']).astype('int64') // 10**9  # Unix timestamp\n",
    "df_optimized['open'] = df_optimized['open'].astype('float32')      # 32-bit float instead of 64-bit\n",
    "df_optimized['high'] = df_optimized['high'].astype('float32')\n",
    "df_optimized['low'] = df_optimized['low'].astype('float32')\n",
    "df_optimized['close'] = df_optimized['close'].astype('float32')\n",
    "df_optimized['volume'] = df_optimized['volume'].astype('uint32')   # Unsigned 32-bit for volume\n",
    "df_optimized['vwap'] = df_optimized['vwap'].astype('float32')\n",
    "df_optimized['transactions'] = df_optimized['transactions'].astype('uint32')  # Unsigned 32-bit\n",
    "\n",
    "print(f\"\\nOptimized data types:\\n{df_optimized.dtypes}\")\n",
    "\n",
    "# Save to parquet with zstd compression\n",
    "test_parquet_path = os.path.join(dir_path, 'SPY_integrity_test.parquet')\n",
    "df_optimized.to_parquet(test_parquet_path, engine='pyarrow', compression='zstd')\n",
    "print(f\"Saved to: {test_parquet_path}\")\n",
    "\n",
    "# Read back from parquet\n",
    "df_loaded = pd.read_parquet(test_parquet_path)\n",
    "print(f\"Loaded parquet shape: {df_loaded.shape}\")\n",
    "print(f\"Loaded data types:\\n{df_loaded.dtypes}\")\n",
    "\n",
    "# Check if shapes match\n",
    "if df_optimized.shape != df_loaded.shape:\n",
    "    print(f\"ERROR: Shape mismatch! Original: {df_optimized.shape}, Loaded: {df_loaded.shape}\")\n",
    "else:\n",
    "    print(\"✓ Shapes match\")\n",
    "\n",
    "# Check if column names match\n",
    "if list(df_optimized.columns) != list(df_loaded.columns):\n",
    "    print(f\"ERROR: Column names mismatch!\")\n",
    "    print(f\"Original columns: {list(df_optimized.columns)}\")\n",
    "    print(f\"Loaded columns: {list(df_loaded.columns)}\")\n",
    "else:\n",
    "    print(\"✓ Column names match\")\n",
    "\n",
    "# Check if data types match\n",
    "dtype_match = True\n",
    "for col in df_optimized.columns:\n",
    "    if df_optimized[col].dtype != df_loaded[col].dtype:\n",
    "        print(f\"ERROR: Data type mismatch for column '{col}'!\")\n",
    "        print(f\"Original: {df_optimized[col].dtype}, Loaded: {df_loaded[col].dtype}\")\n",
    "        dtype_match = False\n",
    "\n",
    "if dtype_match:\n",
    "    print(\"✓ Data types match\")\n",
    "\n",
    "# Check if values match (row by row comparison)\n",
    "print(\"\\nChecking values row by row...\")\n",
    "mismatch_found = False\n",
    "total_rows = len(df_optimized)\n",
    "\n",
    "for idx in range(total_rows):\n",
    "    if idx % 100000 == 0:  # Progress indicator\n",
    "        print(f\"Checking row {idx:,} of {total_rows:,}\")\n",
    "    \n",
    "    for col in df_optimized.columns:\n",
    "        original_val = df_optimized.iloc[idx][col]\n",
    "        loaded_val = df_loaded.iloc[idx][col]\n",
    "        \n",
    "        # Handle NaN values\n",
    "        if pd.isna(original_val) and pd.isna(loaded_val):\n",
    "            continue\n",
    "        \n",
    "        # Handle float precision issues\n",
    "        if isinstance(original_val, (float, np.floating)) and isinstance(loaded_val, (float, np.floating)):\n",
    "            if not np.isclose(original_val, loaded_val, rtol=1e-6, atol=1e-8):\n",
    "                print(f\"ERROR: Value mismatch at row {idx}, column '{col}'!\")\n",
    "                print(f\"Original: {original_val} (type: {type(original_val)})\")\n",
    "                print(f\"Loaded: {loaded_val} (type: {type(loaded_val)})\")\n",
    "                print(f\"Difference: {abs(original_val - loaded_val)}\")\n",
    "                mismatch_found = True\n",
    "                break\n",
    "        else:\n",
    "            # Exact match for integers and other types\n",
    "            if original_val != loaded_val:\n",
    "                print(f\"ERROR: Value mismatch at row {idx}, column '{col}'!\")\n",
    "                print(f\"Original: {original_val} (type: {type(original_val)})\")\n",
    "                print(f\"Loaded: {loaded_val} (type: {type(loaded_val)})\")\n",
    "                mismatch_found = True\n",
    "                break\n",
    "    \n",
    "    if mismatch_found:\n",
    "        break\n",
    "\n",
    "if not mismatch_found:\n",
    "    print(f\"✓ All {total_rows:,} rows checked - no value mismatches found!\")\n",
    "    print(\"✓ DATA INTEGRITY VERIFIED - Parquet format preserves data correctly\")\n",
    "else:\n",
    "    print(\"✗ DATA INTEGRITY FAILED - Found value mismatches\")\n",
    "\n",
    "# Additional statistics comparison\n",
    "print(\"\\n=== STATISTICAL COMPARISON ===\")\n",
    "numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'vwap', 'transactions']\n",
    "for col in numeric_cols:\n",
    "    if col in df_optimized.columns:\n",
    "        orig_sum = df_optimized[col].sum()\n",
    "        loaded_sum = df_loaded[col].sum()\n",
    "        print(f\"{col:12} - Original sum: {orig_sum:15.2f}, Loaded sum: {loaded_sum:15.2f}, Diff: {abs(orig_sum - loaded_sum):.2e}\")\n",
    "\n",
    "# Clean up test file\n",
    "os.remove(test_parquet_path)\n",
    "print(f\"\\nTest file removed: {test_parquet_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
