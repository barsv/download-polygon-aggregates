{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "here claude sonnet 4 compared compression methods for saving and reading 20 years of 1 minute bars for SPY.\n",
    "based on test results, i select parquet with zstd compression.\n",
    "\n",
    "# Parquet Zstd (optimized) shows excellent balance:\n",
    "# - File size: 67.2MB (3rd smallest)\n",
    "# - Read speed: 0.071s (very fast)\n",
    "# - Write speed: 1.062s (reasonable)\n",
    "# - Best compression ratio vs speed trade-off\n",
    "\n",
    "# Comparison with other top performers:\n",
    "\n",
    "# Format          | Type      |    Write |     Read |      Size\n",
    "# -------------------------------------------------------------\n",
    "# Parquet None    | optimized |   1.013s |   0.048s |   101.7MB\n",
    "# Parquet Snappy  | optimized |   0.927s |   0.066s |    85.8MB\n",
    "# Parquet Gzip    | optimized |   4.971s |   0.096s |    61.5MB\n",
    "# Parquet Zstd    | optimized |   1.062s |   0.071s |    67.2MB\n",
    "# Parquet Brotli  | optimized |   9.540s |   0.114s |    57.4MB\n",
    "# Parquet LZ4     | optimized |   0.932s |   0.067s |    85.2MB\n",
    "# Feather         | optimized |   0.186s |   0.069s |    88.6MB\n",
    "\n",
    "# Recommended storage format:\n",
    "def save_optimized_bars(df, filepath):\n",
    "    \"\"\"Save DataFrame with optimized data types and compression\"\"\"\n",
    "    # Optimize data types\n",
    "    df_opt = df.copy()\n",
    "    df_opt['timestamp'] = pd.to_datetime(df_opt['timestamp']).astype('int64') // 10**9\n",
    "    df_opt['open'] = df_opt['open'].astype('float32')\n",
    "    df_opt['high'] = df_opt['high'].astype('float32')\n",
    "    df_opt['low'] = df_opt['low'].astype('float32')\n",
    "    df_opt['close'] = df_opt['close'].astype('float32')\n",
    "    df_opt['volume'] = df_opt['volume'].astype('uint32')\n",
    "    df_opt['vwap'] = df_opt['vwap'].astype('float32')\n",
    "    df_opt['transactions'] = df_opt['transactions'].astype('uint32')\n",
    "    \n",
    "    # Save with Zstd compression\n",
    "    df_opt.to_parquet(filepath, engine='pyarrow', compression='zstd')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow\n",
    "!pip install tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to Python path to import api_key module\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "import settings\n",
    "\n",
    "def test_formats(df, dir_path, timestamp_type):\n",
    "    \"\"\"Test different file formats for saving and reading data\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Define test formats\n",
    "    formats = [\n",
    "        # CSV\n",
    "        ('CSV', lambda: df.to_csv(os.path.join(dir_path, f'SPY_{timestamp_type}.csv'), index=False)),\n",
    "        \n",
    "        # Parquet with different compressions\n",
    "        ('Parquet None', lambda: df.to_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_none.parquet'), \n",
    "                                               engine='pyarrow', compression=None)),\n",
    "        ('Parquet Snappy', lambda: df.to_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_snappy.parquet'), \n",
    "                                                  engine='pyarrow', compression='snappy')),\n",
    "        ('Parquet Gzip', lambda: df.to_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_gzip.parquet'), \n",
    "                                                engine='pyarrow', compression='gzip')),\n",
    "        ('Parquet Zstd', lambda: df.to_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_zstd.parquet'), \n",
    "                                                engine='pyarrow', compression='zstd')),\n",
    "        ('Parquet Brotli', lambda: df.to_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_brotli.parquet'), \n",
    "                                                  engine='pyarrow', compression='brotli')),\n",
    "        ('Parquet LZ4', lambda: df.to_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_lz4.parquet'), \n",
    "                                               engine='pyarrow', compression='lz4')),\n",
    "        \n",
    "        # Other formats\n",
    "        ('Feather', lambda: df.to_feather(os.path.join(dir_path, f'SPY_{timestamp_type}.feather'))),\n",
    "        ('HDF5', lambda: df.to_hdf(os.path.join(dir_path, f'SPY_{timestamp_type}.hdf'), key='SPY', mode='w')),\n",
    "        ('Numpy NPZ', lambda: np.savez_compressed(os.path.join(dir_path, f'SPY_{timestamp_type}.npz'), \n",
    "                                                   data=df.to_numpy(), columns=df.columns.to_numpy())),\n",
    "    ]\n",
    "    \n",
    "    # Test writing speed and file size\n",
    "    print(f\"\\n--- Writing Performance ({timestamp_type} timestamps) ---\")\n",
    "    for name, write_func in formats:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            write_func()\n",
    "            write_time = time.time() - start_time\n",
    "            \n",
    "            # Get file size\n",
    "            if 'CSV' in name:\n",
    "                file_path = os.path.join(dir_path, f'SPY_{timestamp_type}.csv')\n",
    "            elif 'Parquet' in name:\n",
    "                compression = name.split()[-1].lower()\n",
    "                file_path = os.path.join(dir_path, f'SPY_{timestamp_type}_{compression}.parquet')\n",
    "            elif 'Feather' in name:\n",
    "                file_path = os.path.join(dir_path, f'SPY_{timestamp_type}.feather')\n",
    "            elif 'HDF5' in name:\n",
    "                file_path = os.path.join(dir_path, f'SPY_{timestamp_type}.hdf')\n",
    "            elif 'Numpy' in name:\n",
    "                file_path = os.path.join(dir_path, f'SPY_{timestamp_type}.npz')\n",
    "            \n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "            \n",
    "            results.append({\n",
    "                'format': name,\n",
    "                'write_time': write_time,\n",
    "                'file_size': file_size,\n",
    "                'timestamp_type': timestamp_type\n",
    "            })\n",
    "            \n",
    "            print(f\"{name:15} | Write: {write_time:.3f}s | Size: {file_size:.1f}MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:15} | Error: {e}\")\n",
    "    \n",
    "    # Test reading speed\n",
    "    print(f\"\\n--- Reading Performance ({timestamp_type} timestamps) ---\")\n",
    "    read_formats = [\n",
    "        ('CSV', lambda: pd.read_csv(os.path.join(dir_path, f'SPY_{timestamp_type}.csv'))),\n",
    "        ('Parquet None', lambda: pd.read_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_none.parquet'))),\n",
    "        ('Parquet Snappy', lambda: pd.read_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_snappy.parquet'))),\n",
    "        ('Parquet Gzip', lambda: pd.read_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_gzip.parquet'))),\n",
    "        ('Parquet Zstd', lambda: pd.read_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_zstd.parquet'))),\n",
    "        ('Parquet Brotli', lambda: pd.read_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_brotli.parquet'))),\n",
    "        ('Parquet LZ4', lambda: pd.read_parquet(os.path.join(dir_path, f'SPY_{timestamp_type}_lz4.parquet'))),\n",
    "        ('Feather', lambda: pd.read_feather(os.path.join(dir_path, f'SPY_{timestamp_type}.feather'))),\n",
    "        ('HDF5', lambda: pd.read_hdf(os.path.join(dir_path, f'SPY_{timestamp_type}.hdf'), key='SPY')),\n",
    "        ('Numpy NPZ', lambda: load_numpy_to_df(os.path.join(dir_path, f'SPY_{timestamp_type}.npz'))),\n",
    "    ]\n",
    "    \n",
    "    for name, read_func in read_formats:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            df_loaded = read_func()\n",
    "            read_time = time.time() - start_time\n",
    "            \n",
    "            # Update results\n",
    "            for result in results:\n",
    "                if result['format'] == name:\n",
    "                    result['read_time'] = read_time\n",
    "                    break\n",
    "            \n",
    "            print(f\"{name:15} | Read: {read_time:.3f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:15} | Error: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def load_numpy_to_df(file_path):\n",
    "    \"\"\"Helper function to load numpy file back to DataFrame\"\"\"\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    return pd.DataFrame(data['data'], columns=data['columns'])\n",
    "\n",
    "# Main test execution\n",
    "dir_path = os.path.join(settings.ABSOLUTE_DATA_DIR, 'bars', '1minute')\n",
    "all_results = []\n",
    "\n",
    "# Test with datetime objects (parsed dates)\n",
    "print(\"=== TESTING WITH DATETIME TIMESTAMPS ===\")\n",
    "df_datetime = pd.read_csv(os.path.join(dir_path, 'SPY.csv'), parse_dates=['timestamp'])\n",
    "df_datetime['timestamp'] = pd.to_datetime(df_datetime['timestamp'])\n",
    "results_datetime = test_formats(df_datetime, dir_path, \"datetime\")\n",
    "all_results.extend(results_datetime)\n",
    "\n",
    "# Test with string timestamps (original)\n",
    "print(\"\\n=== TESTING WITH STRING TIMESTAMPS ===\")\n",
    "df_string = pd.read_csv(os.path.join(dir_path, 'SPY.csv'))  # Load without parsing dates\n",
    "results_string = test_formats(df_string, dir_path, \"string\")\n",
    "all_results.extend(results_string)\n",
    "\n",
    "# Test with unix timestamps (converted to int64)\n",
    "print(\"\\n=== TESTING WITH UNIX TIMESTAMPS ===\")\n",
    "df_unix = pd.read_csv(os.path.join(dir_path, 'SPY.csv'))\n",
    "df_unix['timestamp'] = pd.to_datetime(df_unix['timestamp']).astype('int64') // 10**9\n",
    "results_unix = test_formats(df_unix, dir_path, \"unix\")\n",
    "all_results.extend(results_unix)\n",
    "\n",
    "# Test with optimized data types for disk storage\n",
    "print(\"\\n=== TESTING WITH OPTIMIZED DATA TYPES ===\")\n",
    "df_optimized = pd.read_csv(os.path.join(dir_path, 'SPY.csv'))\n",
    "\n",
    "# Optimize data types for better compression and storage\n",
    "df_optimized['timestamp'] = pd.to_datetime(df_optimized['timestamp']).astype('int64') // 10**9  # Unix timestamp\n",
    "df_optimized['open'] = df_optimized['open'].astype('float32')      # 32-bit float instead of 64-bit\n",
    "df_optimized['high'] = df_optimized['high'].astype('float32')\n",
    "df_optimized['low'] = df_optimized['low'].astype('float32')\n",
    "df_optimized['close'] = df_optimized['close'].astype('float32')\n",
    "df_optimized['volume'] = df_optimized['volume'].astype('uint32')   # Unsigned 32-bit for volume\n",
    "df_optimized['vwap'] = df_optimized['vwap'].astype('float32')\n",
    "df_optimized['transactions'] = df_optimized['transactions'].astype('uint32')  # Unsigned 32-bit\n",
    "\n",
    "# Show data type info\n",
    "print(\"Original data types:\")\n",
    "print(df_datetime.dtypes)\n",
    "print(f\"\\nOriginal memory usage: {df_datetime.memory_usage(deep=True).sum() / (1024**2):.1f} MB\")\n",
    "\n",
    "print(\"\\nOptimized data types:\")\n",
    "print(df_optimized.dtypes)\n",
    "print(f\"Optimized memory usage: {df_optimized.memory_usage(deep=True).sum() / (1024**2):.1f} MB\")\n",
    "\n",
    "results_optimized = test_formats(df_optimized, dir_path, \"optimized\")\n",
    "all_results.extend(results_optimized)\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(\"\\n=== COMPREHENSIVE SUMMARY ===\")\n",
    "print(f\"{'Format':15} | {'Type':8} | {'Write':>8} | {'Read':>8} | {'Size':>8}\")\n",
    "print(\"-\" * 60)\n",
    "for result in all_results:\n",
    "    if 'read_time' in result:\n",
    "        print(f\"{result['format']:15} | {result['timestamp_type']:8} | \"\n",
    "              f\"{result['write_time']:7.3f}s | {result['read_time']:7.3f}s | \"\n",
    "              f\"{result['file_size']:7.1f}MB\")\n",
    "\n",
    "# Print best performers by category\n",
    "print(\"\\n=== BEST PERFORMERS ===\")\n",
    "# Best for file size\n",
    "best_size = min([r for r in all_results if 'read_time' in r], key=lambda x: x['file_size'])\n",
    "print(f\"Smallest file: {best_size['format']} ({best_size['timestamp_type']}) - {best_size['file_size']:.1f}MB\")\n",
    "\n",
    "# Best for read speed\n",
    "best_read = min([r for r in all_results if 'read_time' in r], key=lambda x: x['read_time'])\n",
    "print(f\"Fastest read: {best_read['format']} ({best_read['timestamp_type']}) - {best_read['read_time']:.3f}s\")\n",
    "\n",
    "# Best balance (read speed / file size ratio)\n",
    "for result in all_results:\n",
    "    if 'read_time' in result:\n",
    "        result['efficiency'] = result['read_time'] / result['file_size']\n",
    "best_balance = min([r for r in all_results if 'read_time' in r], key=lambda x: x['efficiency'])\n",
    "print(f\"Best balance: {best_balance['format']} ({best_balance['timestamp_type']}) - \"\n",
    "      f\"{best_balance['read_time']:.3f}s / {best_balance['file_size']:.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data integrity test - verify that parquet saves and loads data correctly\n",
    "print(\"=== DATA INTEGRITY TEST ===\")\n",
    "\n",
    "# Read original CSV data\n",
    "df_original = pd.read_csv(os.path.join(dir_path, 'SPY.csv'))\n",
    "print(f\"Original CSV shape: {df_original.shape}\")\n",
    "print(f\"Original data types:\\n{df_original.dtypes}\")\n",
    "\n",
    "# Optimize data types (same as before)\n",
    "df_optimized = df_original.copy()\n",
    "df_optimized['timestamp'] = pd.to_datetime(df_optimized['timestamp']).astype('int64') // 10**9  # Unix timestamp\n",
    "df_optimized['open'] = df_optimized['open'].astype('float32')      # 32-bit float instead of 64-bit\n",
    "df_optimized['high'] = df_optimized['high'].astype('float32')\n",
    "df_optimized['low'] = df_optimized['low'].astype('float32')\n",
    "df_optimized['close'] = df_optimized['close'].astype('float32')\n",
    "df_optimized['volume'] = df_optimized['volume'].astype('uint32')   # Unsigned 32-bit for volume\n",
    "df_optimized['vwap'] = df_optimized['vwap'].astype('float32')\n",
    "df_optimized['transactions'] = df_optimized['transactions'].astype('uint32')  # Unsigned 32-bit\n",
    "\n",
    "print(f\"\\nOptimized data types:\\n{df_optimized.dtypes}\")\n",
    "\n",
    "# Save to parquet with zstd compression\n",
    "test_parquet_path = os.path.join(dir_path, 'SPY_integrity_test.parquet')\n",
    "df_optimized.to_parquet(test_parquet_path, engine='pyarrow', compression='zstd')\n",
    "print(f\"Saved to: {test_parquet_path}\")\n",
    "\n",
    "# Read back from parquet\n",
    "df_loaded = pd.read_parquet(test_parquet_path)\n",
    "print(f\"Loaded parquet shape: {df_loaded.shape}\")\n",
    "print(f\"Loaded data types:\\n{df_loaded.dtypes}\")\n",
    "\n",
    "# Check if shapes match\n",
    "if df_optimized.shape != df_loaded.shape:\n",
    "    print(f\"ERROR: Shape mismatch! Original: {df_optimized.shape}, Loaded: {df_loaded.shape}\")\n",
    "else:\n",
    "    print(\"✓ Shapes match\")\n",
    "\n",
    "# Check if column names match\n",
    "if list(df_optimized.columns) != list(df_loaded.columns):\n",
    "    print(f\"ERROR: Column names mismatch!\")\n",
    "    print(f\"Original columns: {list(df_optimized.columns)}\")\n",
    "    print(f\"Loaded columns: {list(df_loaded.columns)}\")\n",
    "else:\n",
    "    print(\"✓ Column names match\")\n",
    "\n",
    "# Check if data types match\n",
    "dtype_match = True\n",
    "for col in df_optimized.columns:\n",
    "    if df_optimized[col].dtype != df_loaded[col].dtype:\n",
    "        print(f\"ERROR: Data type mismatch for column '{col}'!\")\n",
    "        print(f\"Original: {df_optimized[col].dtype}, Loaded: {df_loaded[col].dtype}\")\n",
    "        dtype_match = False\n",
    "\n",
    "if dtype_match:\n",
    "    print(\"✓ Data types match\")\n",
    "\n",
    "# Check if values match (row by row comparison)\n",
    "print(\"\\nChecking values row by row...\")\n",
    "mismatch_found = False\n",
    "total_rows = len(df_optimized)\n",
    "\n",
    "for idx in range(total_rows):\n",
    "    if idx % 100000 == 0:  # Progress indicator\n",
    "        print(f\"Checking row {idx:,} of {total_rows:,}\")\n",
    "    \n",
    "    for col in df_optimized.columns:\n",
    "        original_val = df_optimized.iloc[idx][col]\n",
    "        loaded_val = df_loaded.iloc[idx][col]\n",
    "        \n",
    "        # Handle NaN values\n",
    "        if pd.isna(original_val) and pd.isna(loaded_val):\n",
    "            continue\n",
    "        \n",
    "        # Handle float precision issues\n",
    "        if isinstance(original_val, (float, np.floating)) and isinstance(loaded_val, (float, np.floating)):\n",
    "            if not np.isclose(original_val, loaded_val, rtol=1e-6, atol=1e-8):\n",
    "                print(f\"ERROR: Value mismatch at row {idx}, column '{col}'!\")\n",
    "                print(f\"Original: {original_val} (type: {type(original_val)})\")\n",
    "                print(f\"Loaded: {loaded_val} (type: {type(loaded_val)})\")\n",
    "                print(f\"Difference: {abs(original_val - loaded_val)}\")\n",
    "                mismatch_found = True\n",
    "                break\n",
    "        else:\n",
    "            # Exact match for integers and other types\n",
    "            if original_val != loaded_val:\n",
    "                print(f\"ERROR: Value mismatch at row {idx}, column '{col}'!\")\n",
    "                print(f\"Original: {original_val} (type: {type(original_val)})\")\n",
    "                print(f\"Loaded: {loaded_val} (type: {type(loaded_val)})\")\n",
    "                mismatch_found = True\n",
    "                break\n",
    "    \n",
    "    if mismatch_found:\n",
    "        break\n",
    "\n",
    "if not mismatch_found:\n",
    "    print(f\"✓ All {total_rows:,} rows checked - no value mismatches found!\")\n",
    "    print(\"✓ DATA INTEGRITY VERIFIED - Parquet format preserves data correctly\")\n",
    "else:\n",
    "    print(\"✗ DATA INTEGRITY FAILED - Found value mismatches\")\n",
    "\n",
    "# Additional statistics comparison\n",
    "print(\"\\n=== STATISTICAL COMPARISON ===\")\n",
    "numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'vwap', 'transactions']\n",
    "for col in numeric_cols:\n",
    "    if col in df_optimized.columns:\n",
    "        orig_sum = df_optimized[col].sum()\n",
    "        loaded_sum = df_loaded[col].sum()\n",
    "        print(f\"{col:12} - Original sum: {orig_sum:15.2f}, Loaded sum: {loaded_sum:15.2f}, Diff: {abs(orig_sum - loaded_sum):.2e}\")\n",
    "\n",
    "# Clean up test file\n",
    "os.remove(test_parquet_path)\n",
    "print(f\"\\nTest file removed: {test_parquet_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
