{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas pyarrow scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from data_downloader import download, get_filename\n",
    "from pattern_analysis import get_alpha_lambda, get_rmse, create_window\n",
    "from pattern_searcher import PatternSearcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'AAPL'\n",
    "interval = '5s'\n",
    "year = '2024'\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data to the file on disk.\n",
    "filename = get_filename(ticker, interval, year)\n",
    "if not os.path.exists(filename):\n",
    "\tdownload(ticker, interval, year)\n",
    "# read data from the file on disk.\n",
    "filename = get_filename(ticker, interval, year)\n",
    "df = pd.read_parquet(filename)\n",
    "\n",
    "# show data.\n",
    "# fig = px.line(df[-1000:], y='open', title=f'{ticker} Open Prices')\n",
    "# fig.show()\n",
    "# total_bars = df.shape[0]\n",
    "# print(f'Total bars: {total_bars} ({total_bars:,})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 60 # window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the optimized PatternSearcher with the 'open' column and fixed template length\n",
    "searcher = PatternSearcher(df['open'], template_length=m)\n",
    "\n",
    "# # Print searcher statistics\n",
    "# print(\"PatternSearcher Statistics:\")\n",
    "# stats = searcher.get_stats()\n",
    "# for key, value in stats.items():\n",
    "#     print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "correlations = []\n",
    "# get correlations for N randomly sampled patterns\n",
    "for _ in tqdm(range(N)):\n",
    "    start_index = random.randrange(0, len(df) - m)\n",
    "    pattern = create_window(df, start_index, m)\n",
    "    correlations.append({\n",
    "        'start_index': start_index,\n",
    "        'similar': searcher.get_rs_above(pattern, 0.97)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a single pattern search (for parallel execution)\n",
    "def process_pattern_batch(batch_indices, df_data, template_length, threshold=0.97):\n",
    "    \"\"\"\n",
    "    Process a batch of pattern searches.\n",
    "    This function will be executed in parallel processes.\n",
    "    \n",
    "    Args:\n",
    "        batch_indices: List of start indices for patterns\n",
    "        df_data: DataFrame with the data (passed to avoid pickle issues)\n",
    "        template_length: Length of the pattern window\n",
    "        threshold: Correlation threshold\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for start_index in batch_indices:\n",
    "        pattern = create_window(df_data, start_index, template_length)\n",
    "        results.append({\n",
    "            'start_index': start_index,\n",
    "            'similar': searcher.get_rs_above(pattern, threshold)\n",
    "        })\n",
    "    \n",
    "    return results# Parallel version of pattern search\n",
    "\n",
    "def parallel_pattern_search(N, df, m, threshold=0.97):\n",
    "    \"\"\"\n",
    "    Parallel version of the pattern search loop.\n",
    "    \n",
    "    Args:\n",
    "        N: Number of patterns to analyze\n",
    "        df: DataFrame with data\n",
    "        m: Window size\n",
    "        threshold: Correlation threshold\n",
    "        max_workers: Number of worker processes (None = auto-detect CPU count)\n",
    "    \"\"\"\n",
    "    # Generate all random indices at once for reproducibility\n",
    "    random.seed(42)  # Reset seed for consistency\n",
    "    all_indices = [random.randrange(0, len(df) - m) for _ in range(N)]\n",
    "    \n",
    "    # Split indices into batches for parallel processing\n",
    "    max_workers = 8\n",
    "    batch_size = max(1, N // max_workers)\n",
    "    batches = [all_indices[i:i + batch_size] for i in range(0, N, batch_size)]\n",
    "    \n",
    "    correlations = []\n",
    "    \n",
    "    # Use ProcessPoolExecutor for parallel execution\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all batches\n",
    "        future_to_batch = {\n",
    "            executor.submit(process_pattern_batch, searcher, batch, df, m, threshold): i \n",
    "            for i, batch in enumerate(batches)\n",
    "        }\n",
    "        \n",
    "        # Collect results with progress bar\n",
    "        with tqdm(total=len(batches), desc=\"Processing batches\") as pbar:\n",
    "            for future in as_completed(future_to_batch):\n",
    "                batch_results = future.result()\n",
    "                correlations.extend(batch_results)\n",
    "                pbar.update(1)\n",
    "    \n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parallel version\n",
    "import time\n",
    "\n",
    "N = 1000\n",
    "\n",
    "# Measure time for parallel execution\n",
    "start_time = time.time()\n",
    "correlations_parallel = parallel_pattern_search(N, df, m, threshold=0.97)\n",
    "parallel_time = time.time() - start_time\n",
    "\n",
    "print(f\"Parallel execution completed in {parallel_time:.2f} seconds\")\n",
    "print(f\"Processed {len(correlations_parallel)} patterns\")\n",
    "\n",
    "# Verify results are consistent (optional)\n",
    "total_analyzed_parallel = sum(len(corr['similar']) for corr in correlations_parallel)\n",
    "print(f'Total analyzed points (parallel): {total_analyzed_parallel}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_analyzed = 0\n",
    "# for corr in correlations:\n",
    "#     print(f'index = {corr[\"start_index\"]}, similar patterns: {len(corr[\"similar\"])}')\n",
    "#     total_analyzed += len(corr['similar'])\n",
    "# print(f'Total analyzed points: {total_analyzed}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
