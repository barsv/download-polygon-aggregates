{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8633,
     "status": "ok",
     "timestamp": 1754123154820,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "DSOxAcbV31wl",
    "outputId": "1980035e-514c-4569-c2b6-6c451b67233e"
   },
   "outputs": [],
   "source": [
    "!pip install pandas pyarrow scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1754123154880,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "ejVGyp3V4dPr"
   },
   "outputs": [],
   "source": [
    "# function to download data.\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_filename(ticker, interval, year):\n",
    "    return f'data/{ticker}_{interval}_{year}.parquet'\n",
    "\n",
    "def download(ticker, interval, year):\n",
    "    filename = get_filename(ticker, interval, year)\n",
    "    if year:\n",
    "      # url for seconds\n",
    "      url = f\"https://barsv.com/api/download/file/{ticker}/{year}?format=parquet&interval={interval}\"\n",
    "    else:\n",
    "      # url for minutes+\n",
    "      url = f\"https://barsv.com/api/download/file/{ticker}/{ticker}_{interval}?interval={interval}&format=parquet\"\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Get the total file size from the content-length header\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "    with open(filename, 'wb') as f, tqdm(\n",
    "        desc=filename,\n",
    "        total=total_size,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            size = f.write(data)\n",
    "            bar.update(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52909,
     "status": "ok",
     "timestamp": 1754123207798,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "ws9nB8H15SW3",
    "outputId": "f59ba7f0-5988-4cd2-83c2-6ec0765d2272"
   },
   "outputs": [],
   "source": [
    "ticker = 'AAPL'\n",
    "interval = '5s'\n",
    "year = '2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filename = get_filename(ticker, interval, year)\n",
    "# If file doesn't exist, then download it.\n",
    "if not os.path.exists(filename):\n",
    "\tdownload(ticker, interval, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 7337,
     "status": "ok",
     "timestamp": 1754123215145,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "d07d5bb5",
    "outputId": "08bccf92-6546-4af4-fbcb-1150c0b86717"
   },
   "outputs": [],
   "source": [
    "# plot last points.\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "filename = get_filename(ticker, interval, year)\n",
    "df = pd.read_parquet(filename)\n",
    "\n",
    "fig = px.line(df[-1000:], y='open', title=f'{ticker} Open Prices')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1754123215195,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "bz2MsldEATNV",
    "outputId": "1118d7b0-6bf3-4082-cb91-a48f724911a9"
   },
   "outputs": [],
   "source": [
    "total_bars = df.shape[0]\n",
    "print(f'Total bars: {total_bars} ({total_bars:,})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1754123215204,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "482iou8y0npW"
   },
   "outputs": [],
   "source": [
    "m = 60 # window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1754123215216,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "FkbegGXDKl-L"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_window(df, index, window_size):\n",
    "    # note: astype(np.float64) is needed here because later we\n",
    "    # do `m * sum_a_sq - sum_a**2` that in float32 is always zero\n",
    "    # due to floating-point inaccuracy.\n",
    "    return df.loc[index:index + window_size - 1]['open'].astype(np.float64).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1754123215218,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "a2zYq5MJ8iZM"
   },
   "outputs": [],
   "source": [
    "a0 = create_window(df, 42, m)\n",
    "a = create_window(df, 82997, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1754123215297,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "H75KaQgvKmHI"
   },
   "outputs": [],
   "source": [
    "# implementation of the above formulas.\n",
    "def get_alpha_lambda(a0, a, sum_a0 = None):\n",
    "    ''' sum_a0 is to speed up calculations when we have 1 a0 and many a.'''\n",
    "    m = len(a)\n",
    "    sum_a = a.sum()\n",
    "    if sum_a0 is None:\n",
    "        sum_a0 = a0.sum()\n",
    "    sum_a_sq = (a**2).sum()\n",
    "    sum_a_a0 = (a * a0).sum()\n",
    "\n",
    "    divider = m * sum_a_sq - sum_a**2\n",
    "    if divider == 0:\n",
    "        #print(f'divider: {divider}, m: {m}, sum_a: {sum_a}, sum_a_sq: {sum_a_sq}, a: {a}')\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    alpha = (m * sum_a_a0 - sum_a * sum_a0) / divider\n",
    "    lmbda = (sum_a0 - alpha * sum_a) / m\n",
    "    return alpha, lmbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1754123215360,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "2d009a1e",
    "outputId": "3cf40c3a-0ab8-4b32-829e-0c03a5453617"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_rmse(a0, a):\n",
    "  \"\"\"Calculates the root mean square error of the linear fit between two series.\n",
    "\n",
    "  Args:\n",
    "    a0: The first pandas Series.\n",
    "    a: The second pandas Series.\n",
    "\n",
    "  Returns:\n",
    "    The root mean square error.\n",
    "  \"\"\"\n",
    "  alpha, lmbda = get_alpha_lambda(a0, a)\n",
    "  if np.isnan(alpha):\n",
    "      return 1e9 # a large number\n",
    "  a_transformed = alpha * a + lmbda\n",
    "  return np.sqrt(np.mean((a0 - a_transformed)**2))\n",
    "\n",
    "print(f'get_rmse(a0, a): {get_rmse(a0, a)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2435,
     "status": "ok",
     "timestamp": 1754123217794,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "cefaac66"
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "class PatternSearcher:\n",
    "    \"\"\"\n",
    "    An optimized class to pre-calculate cumulative sums and rolling window statistics\n",
    "    for pattern searching using Normalized Cross-Correlation with fixed template length.\n",
    "    \"\"\"\n",
    "    def __init__(self, series, template_length):\n",
    "        \"\"\"\n",
    "        Initializes the PatternSearcher with a time series and fixed template length.\n",
    "        Precomputes all rolling window statistics for efficiency.\n",
    "\n",
    "        Args:\n",
    "            series: The time series data (pandas Series or NumPy array).\n",
    "            template_length: Fixed length of all templates that will be searched.\n",
    "        \"\"\"\n",
    "        # Convert the series to a NumPy array with float64 dtype for numerical stability\n",
    "        self.series = np.asarray(series, dtype=np.float64)\n",
    "        self.n = len(self.series)\n",
    "        self.m = template_length\n",
    "        \n",
    "        if self.m > self.n:\n",
    "            raise ValueError(\"Template length is greater than series length.\")\n",
    "\n",
    "        # Pre-calculate cumulative sums\n",
    "        self.cum_sum_x = np.cumsum(self.series)\n",
    "        self.cum_sum_x_sq = np.cumsum(self.series**2)\n",
    "        \n",
    "        # --- Pre-calculate all rolling window statistics for the series ---\n",
    "        # Rolling sum of x for each window\n",
    "        self.sum_x = self.cum_sum_x[self.m-1:] - np.concatenate(([0], self.cum_sum_x[:-self.m]))\n",
    "        # Rolling sum of x^2 for each window  \n",
    "        self.sum_x_sq = self.cum_sum_x_sq[self.m-1:] - np.concatenate(([0], self.cum_sum_x_sq[:-self.m]))\n",
    "        \n",
    "        # Denominator term for x for each window (precomputed)\n",
    "        self.denom_x = self.m * self.sum_x_sq - self.sum_x**2\n",
    "        # Avoid division by zero for windows with no variance\n",
    "        self.denom_x[self.denom_x <= 1e-10] = 1  # Set to 1 to avoid NaN, result will be 0 anyway\n",
    "        \n",
    "        # Number of valid windows\n",
    "        self.num_windows = len(self.sum_x)\n",
    "\n",
    "    def search(self, template):\n",
    "        \"\"\"\n",
    "        Finds correlation coefficients for a template pattern within the time series\n",
    "        using optimized Normalized Cross-Correlation.\n",
    "\n",
    "        Args:\n",
    "            template: The pattern template to search for (pandas Series or NumPy array).\n",
    "                     Must have length equal to template_length specified in constructor.\n",
    "\n",
    "        Returns:\n",
    "            NumPy array of correlation coefficients for all valid windows,\n",
    "            or None if the template has zero variance.\n",
    "        \"\"\"\n",
    "        # Convert the template to a NumPy array with float64 dtype\n",
    "        template = np.asarray(template, dtype=np.float64)\n",
    "        \n",
    "        if len(template) != self.m:\n",
    "            raise ValueError(f\"Template length {len(template)} does not match expected length {self.m}\")\n",
    "\n",
    "        # --- Calculate terms for the template (y) ---\n",
    "        sum_y = template.sum()\n",
    "        sum_y_sq = (template**2).sum()\n",
    "        # Denominator term for y, which is constant\n",
    "        denom_y = self.m * sum_y_sq - sum_y**2\n",
    "\n",
    "        if denom_y <= 1e-10:  # Use a small epsilon for floating point comparison\n",
    "            print(\"Template has zero variance, cannot perform matching.\")\n",
    "            return None\n",
    "\n",
    "        # --- Calculate cross-correlation term sum(xy) using FFT ---\n",
    "        # This is the sum(x*y) term, calculated efficiently using FFT\n",
    "        sum_xy = signal.fftconvolve(self.series, template[::-1], mode='valid')\n",
    "\n",
    "        # --- Calculate Correlation Coefficient (r) for all windows ---\n",
    "        # Numerator term (using precomputed sum_x)\n",
    "        num = self.m * sum_xy - self.sum_x * sum_y\n",
    "        # Denominator term (using precomputed denom_x)\n",
    "        denom = np.sqrt(self.denom_x * denom_y)\n",
    "\n",
    "        # Handle potential division by zero in denom\n",
    "        r = np.zeros_like(num, dtype=np.float64)  # Initialize r with zeros\n",
    "        non_zero_denom_mask = denom > 1e-10  # Create a mask for non-zero denominators\n",
    "        # Calculate r only where denom is not zero\n",
    "        r[non_zero_denom_mask] = num[non_zero_denom_mask] / denom[non_zero_denom_mask]\n",
    "\n",
    "        return r\n",
    "    \n",
    "    def get_rs_above(self, template, r_limit):\n",
    "        \"\"\"\n",
    "        Get correlation results above the given r_limit.\n",
    "        \n",
    "        Args:\n",
    "            template: The pattern template to search for.\n",
    "            r_limit: The minimum correlation threshold (can be negative for negative correlations).\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples (index, correlation_value) where absolute correlation >= r_limit,\n",
    "            or None if search failed.\n",
    "        \"\"\"\n",
    "        r = self.search(template)\n",
    "        if r is None:\n",
    "            return None\n",
    "        \n",
    "        # Find indices where absolute correlation is above the limit\n",
    "        above_limit_mask = np.abs(r) >= r_limit\n",
    "        above_limit_indices = np.where(above_limit_mask)[0]\n",
    "        above_limit_values = r[above_limit_indices]\n",
    "        \n",
    "        # Return as list of tuples (index, correlation_value)\n",
    "        return list(zip(above_limit_indices, above_limit_values))\n",
    "\n",
    "    def get_sorted_r(self, template, return_top_k=None):\n",
    "        \"\"\"\n",
    "        Get sorted correlation results with optional optimization for top-K results.\n",
    "        \n",
    "        Args:\n",
    "            template: The pattern template to search for.\n",
    "            return_top_k: If specified, return only the top K results using argpartition\n",
    "                         for better performance. If None, return all results sorted.\n",
    "        \n",
    "        Returns:\n",
    "            List of tuples (index, correlation_value) sorted by absolute correlation.\n",
    "        \"\"\"\n",
    "        r = self.search(template)\n",
    "        if r is None:\n",
    "            return None\n",
    "            \n",
    "        # OPTIMIZATION: Use argpartition for top-K selection instead of full sorting\n",
    "        if return_top_k is not None and return_top_k < len(r):\n",
    "            # Use argpartition to find top-K efficiently (O(n) vs O(n log n))\n",
    "            abs_r = np.abs(r)\n",
    "            top_k_indices = np.argpartition(abs_r, -return_top_k)[-return_top_k:]\n",
    "            # Sort only the top-K results\n",
    "            sorted_within_top = np.argsort(abs_r[top_k_indices])[::-1]\n",
    "            final_indices = top_k_indices[sorted_within_top]\n",
    "            top_r_values = r[final_indices]\n",
    "            return list(zip(final_indices, top_r_values))\n",
    "        else:\n",
    "            # Full sorting (original behavior)\n",
    "            top_indices = np.argsort(np.abs(r))[::-1]\n",
    "            top_r_values = r[top_indices]\n",
    "            return list(zip(top_indices, top_r_values))\n",
    "            \n",
    "    def get_stats(self):\n",
    "        \"\"\"\n",
    "        Get statistics about the searcher for debugging/monitoring.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with searcher statistics.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'series_length': self.n,\n",
    "            'template_length': self.m, \n",
    "            'num_windows': self.num_windows,\n",
    "            'memory_usage_mb': (\n",
    "                self.series.nbytes + \n",
    "                self.cum_sum_x.nbytes + \n",
    "                self.cum_sum_x_sq.nbytes +\n",
    "                self.sum_x.nbytes +\n",
    "                self.sum_x_sq.nbytes +\n",
    "                self.denom_x.nbytes\n",
    "            ) / 1024**2\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1754123218529,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "El7KLJ0Fz8EJ"
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Instantiate the optimized PatternSearcher with the 'open' column and fixed template length\n",
    "searcher = PatternSearcher(df['open'], template_length=m)\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Print searcher statistics\n",
    "print(\"PatternSearcher Statistics:\")\n",
    "stats = searcher.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72484,
     "status": "ok",
     "timestamp": 1754123291020,
     "user": {
      "displayName": "Stanislav Baranov",
      "userId": "01012316052495130455"
     },
     "user_tz": -180
    },
    "id": "DF5T0musLs3Y",
    "outputId": "5d32f650-e4c8-4d27-e255-ab3f762e6983"
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "correlations = []\n",
    "# get correlations for N randomly sampled patterns\n",
    "for _ in tqdm(range(N)):\n",
    "    start_index = random.randrange(0, len(df) - m)\n",
    "    pattern = create_window(df, start_index, m)\n",
    "    correlations.append({\n",
    "        'start_index': start_index,\n",
    "        'similar': searcher.get_rs_above(pattern, 0.95)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_analyzed = 0\n",
    "for corr in correlations:\n",
    "    print(f'index = {corr[\"start_index\"]}, similar patterns: {len(corr[\"similar\"])}')\n",
    "    total_analyzed += len(corr['similar'])\n",
    "print(f'Total analyzed points: {total_analyzed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find correlation item with index 869145\n",
    "index_to_find = 869145\n",
    "similar_points = []\n",
    "for corr in correlations:\n",
    "    if corr['start_index'] == index_to_find:\n",
    "        print(f'Found correlation at index {index_to_find}: {corr}')\n",
    "        similar_points = corr['similar']\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(similar_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_points[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = create_window(df, 261, m)\n",
    "a = create_window(df, 1051, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_rescaled(a0, a):\n",
    "    alpha, lmbda = get_alpha_lambda(a0, a)\n",
    "    print(f'alpha = {alpha}')\n",
    "    print(f'lambda = {lmbda}')\n",
    "\n",
    "    # Apply the transformation\n",
    "    a_transformed = alpha * a + lmbda\n",
    "\n",
    "    # Plot the original and transformed data\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=a0, name='a0'))\n",
    "    fig.add_trace(go.Scatter(y=a_transformed, name='a (transformed)'))\n",
    "    fig.add_annotation(\n",
    "        text=\"Fig. 3. Here a is rescaled to match a0. NOTE: negative alpha flips a.\",\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.5, y=-0.20,\n",
    "        showarrow=False,\n",
    "        font=dict(size=16),\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_rescaled(a0, a)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMv3Jx1ZMOlYe7MwanOBqQw",
   "provenance": [
    {
     "file_id": "1G0OrWP0ffKRkHB_MjGEeIWPPFdcS4sp_",
     "timestamp": 1753989025804
    }
   ]
  },
  "kernelspec": {
   "display_name": "venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
