{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas pyarrow scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from data_downloader import download, get_filename\n",
    "from pattern_analysis import get_alpha_lambda, get_rmse, create_window\n",
    "from pattern_searcher import PatternSearcher\n",
    "from trailing_stop_loss import generate_stop_loss_levels\n",
    "from trailing_stop_loss import calculate_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2024'\n",
    "ticker = 'AAPL'\n",
    "interval = '5s'\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "# download data to the file on disk.\n",
    "filename = get_filename(ticker, interval, year)\n",
    "if not os.path.exists(filename):\n",
    "\tdownload(ticker, interval, year)\n",
    "# read data from the file on disk.\n",
    "df = pd.read_parquet(filename)\n",
    "\n",
    "# show data.\n",
    "# fig = px.line(df[-1000:], y='open', title=f'{ticker} Open Prices')\n",
    "# fig.show()\n",
    "total_bars = df.shape[0]\n",
    "print(f'Total bars: {total_bars} ({total_bars:,})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 60 # window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the optimized PatternSearcher with the 'open' column and fixed template length\n",
    "searcher = PatternSearcher(df['open'], template_length=m)\n",
    "\n",
    "# # Print searcher statistics\n",
    "# print(\"PatternSearcher Statistics:\")\n",
    "# stats = searcher.get_stats()\n",
    "# for key, value in stats.items():\n",
    "#     print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sl = 0.3\n",
    "sl_steps = 30\n",
    "stop_loss_percents, stop_loss_levels = generate_stop_loss_levels(max_sl, sl_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsl_profits = calculate_pl(df, stop_loss_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = np.arange(len(df['open']), dtype=np.int64)\n",
    "np.random.shuffle(all_indices)\n",
    "all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "seen = np.zeros(len(df), dtype=int)\n",
    "start_from = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "tqdm.write(f'starting from {start_from}')\n",
    "for i in tqdm(range(start_from, start_from + N)):\n",
    "    start_index = all_indices[i]\n",
    "    # # (optional) optimization to not analyse similar points\n",
    "    # if seen[start_index] > 0:\n",
    "    #     continue\n",
    "    pattern = create_window(df, start_index, m)\n",
    "    r = searcher.search(pattern) # correlations\n",
    "    r_limit = 0.98\n",
    "    above_limit_mask = np.abs(r) > r_limit\n",
    "    # start points of similar windows\n",
    "    similar_starts = np.where(above_limit_mask)[0]\n",
    "    if len(similar_starts) > 100:\n",
    "        # entry points for trading (after pattern ends)\n",
    "        entry_points = similar_starts + m\n",
    "        profits_means = tsl_profits[:, entry_points].mean(axis=1)\n",
    "        profits_max = max(profits_means)\n",
    "        profits_stds = tsl_profits[:, entry_points].std(axis=1)\n",
    "        results.append((start_index, profits_max, len(similar_starts), profits_means, profits_stds))\n",
    "    # if there is only one point with r > r_limit then it's the start_index with r == 1\n",
    "    if len(similar_starts) == 1:\n",
    "        # only one match (the pattern itself), mark just this index\n",
    "        seen[start_index] = 1\n",
    "    else:\n",
    "        # multiple matches, mark all highly correlated patterns\n",
    "        high_corr_mask = np.abs(r) > 0.98\n",
    "        high_corr_starts = np.where(high_corr_mask)[0]\n",
    "        seen[high_corr_starts] = 1\n",
    "    start_from += 1\n",
    "for best in sorted(results, key=lambda x: x[1], reverse=True)[:3]:\n",
    "    print(f'start_index = {best[0]}, max mean profit = {best[1]}, similar points = {best[2]}')\n",
    "    print(f'mean tsl profits:\\n{best[3]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to process a single pattern search (for parallel execution)\n",
    "# def process_pattern_batch(batch_indices, df_data, template_length, threshold=0.97):\n",
    "#     \"\"\"\n",
    "#     Process a batch of pattern searches.\n",
    "#     This function will be executed in parallel processes.\n",
    "    \n",
    "#     Args:\n",
    "#         batch_indices: List of start indices for patterns\n",
    "#         df_data: DataFrame with the data (passed to avoid pickle issues)\n",
    "#         template_length: Length of the pattern window\n",
    "#         threshold: Correlation threshold\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "#     for start_index in batch_indices:\n",
    "#         pattern = create_window(df_data, start_index, template_length)\n",
    "#         results.append({\n",
    "#             'start_index': start_index,\n",
    "#             'similar': searcher.get_rs_above(pattern, threshold)\n",
    "#         })\n",
    "    \n",
    "#     return results# Parallel version of pattern search\n",
    "\n",
    "# def parallel_pattern_search(N, df, m, threshold=0.97):\n",
    "#     \"\"\"\n",
    "#     Parallel version of the pattern search loop.\n",
    "    \n",
    "#     Args:\n",
    "#         N: Number of patterns to analyze\n",
    "#         df: DataFrame with data\n",
    "#         m: Window size\n",
    "#         threshold: Correlation threshold\n",
    "#         max_workers: Number of worker processes (None = auto-detect CPU count)\n",
    "#     \"\"\"\n",
    "#     # Generate all random indices at once for reproducibility\n",
    "#     random.seed(42)  # Reset seed for consistency\n",
    "#     all_indices = [random.randrange(0, len(df) - m) for _ in range(N)]\n",
    "    \n",
    "#     # Split indices into batches for parallel processing\n",
    "#     max_workers = 8\n",
    "#     batch_size = max(1, N // max_workers)\n",
    "#     batches = [all_indices[i:i + batch_size] for i in range(0, N, batch_size)]\n",
    "    \n",
    "#     correlations = []\n",
    "    \n",
    "#     # Use ProcessPoolExecutor for parallel execution\n",
    "#     with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         # Submit all batches\n",
    "#         future_to_batch = {\n",
    "#             executor.submit(process_pattern_batch, searcher, batch, df, m, threshold): i \n",
    "#             for i, batch in enumerate(batches)\n",
    "#         }\n",
    "        \n",
    "#         # Collect results with progress bar\n",
    "#         with tqdm(total=len(batches), desc=\"Processing batches\") as pbar:\n",
    "#             for future in as_completed(future_to_batch):\n",
    "#                 batch_results = future.result()\n",
    "#                 correlations.extend(batch_results)\n",
    "#                 pbar.update(1)\n",
    "    \n",
    "#     return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run parallel version\n",
    "# import time\n",
    "\n",
    "# N = 1000\n",
    "\n",
    "# # Measure time for parallel execution\n",
    "# start_time = time.time()\n",
    "# correlations_parallel = parallel_pattern_search(N, df, m, threshold=0.97)\n",
    "# parallel_time = time.time() - start_time\n",
    "\n",
    "# print(f\"Parallel execution completed in {parallel_time:.2f} seconds\")\n",
    "# print(f\"Processed {len(correlations_parallel)} patterns\")\n",
    "\n",
    "# # Verify results are consistent (optional)\n",
    "# total_analyzed_parallel = sum(len(corr['similar']) for corr in correlations_parallel)\n",
    "# print(f'Total analyzed points (parallel): {total_analyzed_parallel}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_analyzed = 0\n",
    "# for corr in correlations:\n",
    "#     print(f'index = {corr[\"start_index\"]}, similar patterns: {len(corr[\"similar\"])}')\n",
    "#     total_analyzed += len(corr['similar'])\n",
    "# print(f'Total analyzed points: {total_analyzed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# TEST ONE RESULT NEXT YEAR!!!11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2024\n",
    "filename = get_filename(ticker, interval, year)\n",
    "if not os.path.exists(filename):\n",
    "\tdownload(ticker, interval, year)\n",
    "# read data from the file on disk.\n",
    "df = pd.read_parquet(filename)\n",
    "\n",
    "# # show data.\n",
    "# fig = px.line(df[-1000:], y='open', title=f'{ticker} Open Prices')\n",
    "# fig.show()\n",
    "total_bars = df.shape[0]\n",
    "print(f'Total bars: {total_bars} ({total_bars:,})')\n",
    "\n",
    "start_index = 834009 # 834009 # 834009\n",
    "temaplate = create_window(df, start_index, m)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=temaplate))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2025\n",
    "filename = get_filename(ticker, interval, year)\n",
    "if not os.path.exists(filename):\n",
    "\tdownload(ticker, interval, year)\n",
    "# read data from the file on disk.\n",
    "df = pd.read_parquet(filename)\n",
    "\n",
    "# # show data.\n",
    "# fig = px.line(df[-1000:], y='open', title=f'{ticker} Open Prices')\n",
    "# fig.show()\n",
    "total_bars = df.shape[0]\n",
    "print(f'Total bars: {total_bars} ({total_bars:,})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = PatternSearcher(df['open'], template_length=m)\n",
    "r = searcher.search(temaplate)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsl_profits = calculate_pl(df, stop_loss_levels)\n",
    "tsl_profits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_limit = 0.98\n",
    "above_limit_mask = np.abs(r) > r_limit\n",
    "# start points of similar windows\n",
    "similar_starts = np.where(above_limit_mask)[0]\n",
    "\n",
    "len(similar_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entry points for trading (after pattern ends)\n",
    "entry_points = similar_starts + m\n",
    "profits_means = tsl_profits[:, entry_points].mean(axis=1)\n",
    "profits_max = max(profits_means)\n",
    "profits_stds = tsl_profits[:, entry_points].std(axis=1)\n",
    "print(f'start_index = {start_index}, max mean profit = {profits_max}, similar points = {len(similar_starts)}')\n",
    "# print(f'mean tsl profits:\\n{list(zip(stop_loss_percents, profits_means))}')\n",
    "for sl_pct, profit_mean in zip(stop_loss_percents, profits_means):\n",
    "    print(f'  {sl_pct:.3f}: {profit_mean:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
