{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "TL;DR: i created download_trades.py from this notebook.\n",
    "\n",
    "long story:\n",
    "\n",
    "here is what i've found.\n",
    "\n",
    "1) rclone doesn't compare checksums for polygon flat files. proof:\n",
    "\n",
    "```\n",
    "> rclone copy s3polygon:flatfiles/us_stocks_sip/trades_v1/2025/06/2025-06-12.csv.gz . --log-level DEBUG --progress --checksum\n",
    "...\n",
    "2025/06/15 12:13:14 DEBUG : 2025-06-12.csv.gz: Src hash empty - aborting Dst hash check\n",
    "...\n",
    "```\n",
    "\n",
    "2) files in s3 have ETag that contains checksum. but... files in s3 are stored in chuncks. so it's not possible to just calculate md5 of a local file and compare it to ETag, you have to split the local file on chuncks first. join md5's of all chunks and get md5 of the join. the ETag has the number of chunks in the end, for example for the 1.8G file `2025-06-12.csv.gz` the ETag is `b8744a0cf028db0fb5a01f6b89a2c853-18`. here 18 means 18 chunks.\n",
    "\n",
    "3) the size of chunks is not a standard is s3 and can be configured by the guy who uploads the file (no proof, chatGPT said and i believe it).\n",
    "i found out that for polygon files the chunk size is 100Mb. \n",
    "i'm not sure if it's the same size for all files, but so far I checked it for 2Gb files, for files less than 100mb and for 109mb file.\n",
    "the checksum check seems to be working fine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup client for AWS S3\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to Python path to import api_key module\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "import api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key_id = api_key.read_api_key_id()\n",
    "aws_secret_access_key = api_key.read_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# Initialize a session using your credentials\n",
    "session = boto3.Session(\n",
    "  aws_access_key_id,\n",
    "  aws_secret_access_key,\n",
    ")\n",
    "\n",
    "# Create a client with your session and specify the endpoint\n",
    "s3 = session.client(\n",
    "  's3',\n",
    "  endpoint_url='https://files.polygon.io',\n",
    "  config=Config(signature_version='s3v4'),\n",
    ")\n",
    "\n",
    "# List Example\n",
    "# Initialize a paginator for listing objects\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "\n",
    "# Choose the appropriate prefix depending on the data you need:\n",
    "# - 'global_crypto' for global cryptocurrency data\n",
    "# - 'global_forex' for global forex data\n",
    "# - 'us_indices' for US indices data\n",
    "# - 'us_options_opra' for US options (OPRA) data\n",
    "# - 'us_stocks_sip' for US stocks (SIP) data\n",
    "prefix = 'us_stocks_sip/trades_v1'  # Example: Change this prefix to match your data need\n",
    "\n",
    "# List objects using the selected prefix\n",
    "for page in paginator.paginate(Bucket='flatfiles', Prefix=prefix):\n",
    "  for obj in page['Contents']:\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the bucket name\n",
    "bucket_name = 'flatfiles'\n",
    "\n",
    "# Specify the S3 object key name\n",
    "object_key = 'us_stocks_sip/trades_v1/2025/06/2025-06-12.csv.gz'\n",
    "\n",
    "# Specify the local file name and path to save the downloaded file\n",
    "# This splits the object_key string by '/' and takes the last segment as the file name\n",
    "local_file_name = object_key.split('/')[-1]\n",
    "\n",
    "# This constructs the full local file path\n",
    "local_file_path = './' + local_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ETag without downloading the file\n",
    "response = s3.head_object(Bucket=bucket_name, Key=object_key)\n",
    "etag = response['ETag'].strip('\"')  # Remove quotes around ETag\n",
    "print(f\"ETag for {object_key}: {etag}\")\n",
    "\n",
    "# ETag is typically MD5 hash for single-part uploads\n",
    "# You can compare this with the MD5 of your local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file\n",
    "# s3.download_file(bucket_name, object_key, local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def calculate_multipart_etag(file_path, chunk_size=8 * 1024 * 1024):\n",
    "    \"\"\"Calculate ETag for multipart upload to match S3's calculation\"\"\"\n",
    "    md5s = []\n",
    "    with open(file_path, 'rb') as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            md5s.append(hashlib.md5(chunk).digest())\n",
    "    if len(md5s) == 1:\n",
    "        # Single part upload - return simple MD5\n",
    "        return md5s[0].hex()\n",
    "    else:\n",
    "        # Multipart upload - combine MD5s and add part count\n",
    "        combined_md5 = hashlib.md5(b''.join(md5s)).hexdigest()\n",
    "        return f\"{combined_md5}-{len(md5s)}\"\n",
    "\n",
    "def equal_md5(bucket_name, object_key, local_file_path, s3_client):\n",
    "    \"\"\"Verify that local file and the bucket object have the same size and MD5 checksum\"\"\"\n",
    "    print(f\"Verifying {local_file_path} against {bucket_name}/{object_key}\")\n",
    "    # Get remote file metadata\n",
    "    response = s3_client.head_object(Bucket=bucket_name, Key=object_key)\n",
    "    remote_etag = response['ETag'].strip('\"')\n",
    "    remote_size = response['ContentLength']\n",
    "    # Check file sizes first (quick check)\n",
    "    local_size = os.path.getsize(local_file_path)\n",
    "    if remote_size != local_size:\n",
    "        print(\"✗ File sizes do not match\")\n",
    "        return False\n",
    "    print(\"✓ File sizes match\")\n",
    "    # For multipart uploads use known chunk sizes\n",
    "    if '-' in remote_etag:\n",
    "        chunk_size = 100 * 1024 * 1024\n",
    "        calculated_etag = calculate_multipart_etag(local_file_path, chunk_size)\n",
    "        if calculated_etag == remote_etag:\n",
    "            print(f\"✓ File verified with {chunk_size // (1024*1024)}MB chunks\")\n",
    "            return True\n",
    "        print(\"✗ Could not verify multipart file integrity\")\n",
    "        return False\n",
    "    else:\n",
    "        # Single part - simple MD5\n",
    "        local_md5 = calculate_multipart_etag(local_file_path)\n",
    "        if local_md5 == remote_etag:\n",
    "            print(\"✓ File verified (single part)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"✗ File verification failed\")\n",
    "            return False\n",
    "\n",
    "is_valid = equal_md5(bucket_name, object_key, local_file_path, s3)\n",
    "is_valid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
