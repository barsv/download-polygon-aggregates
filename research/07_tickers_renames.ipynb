{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "in this notebook i'm trying to get renames for all (active and inactive) tickers.\n",
    "\n",
    "what i've found so far.\n",
    "\n",
    "1) there are a lot of cases when polygon doesn't provide figi for a ticker. and this happens not only for tickers that \n",
    "    were delisted before a figi was assigned. it happens even for active companies where tradingview shows their figi.\n",
    "    for example, XP BBG00QVJYGM9 is trading already 5 years starting from 2020. the ticker XP is present but no figi.\n",
    "\n",
    "2) for 1 figi sometimes there are more than one ticker at the same time. some examples:\n",
    "\n",
    "```\n",
    "    2025-05-30,VLGE.A,BBG000BWGK40,True,          ,usd,,2016-05-18T00:00:00Z,us,stocks,VILLAGE SUPER MARKET CL-A NEW,XNAS,\n",
    "    2025-05-30,VLGEA ,BBG000BWGK40,True,0000103595,usd,,2025-05-30T00:00:00Z,us,stocks,Village Super Market         ,XNAS,CS\n",
    "```\n",
    "\n",
    "\n",
    "    here tickers will be the same if '.' is removed but look at the next case:\n",
    "\n",
    "```\n",
    "    2012-04-24,ABD  ,BBG000J06K07,True,0000712034,usd,,2012-04-26T00:00:00Z,us,stocks,ACCO BRANDS CORPORATION,XNYS,CS\n",
    "    2012-04-24,ACCOw,BBG000J06K07,True,0000712034,usd,,2012-04-30T00:00:00Z,us,stocks,ACCO BRANDS CORPORATION W.I.,XNYS,\n",
    "```\n",
    "\n",
    "    usually warrants are with 'w' suffix but here the whole ticker is different.\n",
    "\n",
    "3) sometimes there are complete duplicates. for example, \n",
    "https://api.polygon.io/v3/reference/tickers?ticker=UST&market=stocks&date=2025-06-01&active=true&order=asc&limit=100&sort=ticker&apiKey= \n",
    "\n",
    "returns:\n",
    "\n",
    "```\n",
    "    {\n",
    "    \"results\": [\n",
    "        {\n",
    "        \"ticker\": \"UST\",\n",
    "        \"name\": \"ProShares Ultra 7-10 Year Treasury\",\n",
    "        \"market\": \"stocks\",\n",
    "        \"locale\": \"us\",\n",
    "        \"primary_exchange\": \"ARCX\",\n",
    "        \"type\": \"ETF\",\n",
    "        \"active\": true,\n",
    "        \"currency_name\": \"usd\",\n",
    "        \"composite_figi\": \"BBG000BH4371\",\n",
    "        \"share_class_figi\": \"BBG001SK30C5\",\n",
    "        \"last_updated_utc\": \"2024-09-23T00:00:00Z\"\n",
    "        },\n",
    "        {\n",
    "        \"ticker\": \"UST\",\n",
    "        \"name\": \"ProShares Ultra 7-10 Year Treasury\",\n",
    "        \"market\": \"stocks\",\n",
    "        \"locale\": \"us\",\n",
    "        \"primary_exchange\": \"ARCX\",\n",
    "        \"type\": \"ETF\",\n",
    "        \"active\": true,\n",
    "        \"currency_name\": \"usd\",\n",
    "        \"cik\": \"0001373525\",\n",
    "        \"composite_figi\": \"BBG000BH4371\",\n",
    "        \"share_class_figi\": \"BBG001SK30C5\",\n",
    "        \"last_updated_utc\": \"2025-06-02T00:00:00Z\"\n",
    "        }\n",
    "    ],\n",
    "    \"status\": \"OK\",\n",
    "    \"request_id\": \"54d441e0bb407fc21739f13b8a5af347\",\n",
    "    \"count\": 2\n",
    "    }\n",
    "```\n",
    "\n",
    "    the only difference is cik.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "# Add the parent directory to Python path to import api_key module\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_dir = os.path.join(settings.ABSOLUTE_DATA_DIR, 'tickers')\n",
    "active_tickers_file = os.path.join(tickers_dir, 'tickers_history_active.csv')\n",
    "inactive_tickers_file = os.path.join(tickers_dir, 'tickers_history_inactive.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(active_tickers_file, newline='', encoding='utf-8') as fin:\n",
    "    print(fin.readline())  # Print header\n",
    "    print(fin.readline())  # the first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "what will not work:\n",
    "- inside a given day figi is not unique, because it's possible to have several tickers for the same figi.\n",
    "- it's not a good idea to try to \"compress\" history by just removing duplicates in the rows because such \n",
    "  \"compressed\" data will loose information about when a ticker disappeared from history.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "algorithm:\n",
    "1. loop through active_tickers_file\n",
    "2. collect all rows for the first day.\n",
    "   need to check here that there are no duplicate tickers within one day.\n",
    "   create a dictionary that maps a ticker to its row.\n",
    "3. repeat (2) for the next day.\n",
    "   now loop over all tickers in the current day and compare them to tickers in the previous day.\n",
    "   if there is a change in (composite_figi, active, cik, delisted_utc, name, primary_exchange, type)\n",
    "   then log it into history as history['figi']['ticker']['date']=row.\n",
    "   if there are new tickers then they will be added as well.\n",
    "   when processing a row in the active_today remove it from active_yesterday.\n",
    "   in the end active_yesterday will have tickers that are missing today. add them to the history.\n",
    "   history['figi']['ticker']['date']=null\n",
    "'''\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "history = {}\n",
    "counter = 0\n",
    "active_today = {}\n",
    "active_yesterday = {}\n",
    "\n",
    "with open(active_tickers_file, newline='', encoding='utf-8') as fin:\n",
    "    reader = csv.DictReader(fin)\n",
    "    for row in reader:\n",
    "        counter += 1\n",
    "        if counter % (1000 * 1000) == 0:\n",
    "            print(f\"Processed {counter} rows\")\n",
    "        figi  = row[\"composite_figi\"]\n",
    "        if not figi:\n",
    "            # Skip rows without FIGI\n",
    "            continue\n",
    "        ticker= row[\"ticker\"]\n",
    "        date  = datetime.fromisoformat(row[\"date\"]).date()\n",
    "        if figi not in history:\n",
    "            # first time encountering this FIGI\n",
    "            history[figi] = { ticker: { 'first_active' : date } }\n",
    "        else:\n",
    "            f = history[figi]\n",
    "            if ticker not in f:\n",
    "                # first time encountering this ticker for this FIGI\n",
    "                f[ticker] = { 'first_active' : date }\n",
    "            else:\n",
    "                # known ticker for this FIGI\n",
    "                f[ticker]['last_active'] = date\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history['BBG000MM2P62']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history['BBG000J06K07']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now analyze inactive tickers\n",
    "surprise = {}\n",
    "\n",
    "with open(inactive_tickers_file, newline='', encoding='utf-8') as fin:\n",
    "    reader = csv.DictReader(fin)\n",
    "    for row in reader:\n",
    "        counter += 1\n",
    "        if counter % (1000 * 1000) == 0:\n",
    "            print(f\"Processed {counter} rows\")\n",
    "        figi  = row[\"composite_figi\"]\n",
    "        if not figi:\n",
    "            # Skip rows without FIGI\n",
    "            continue\n",
    "        ticker= row[\"ticker\"]\n",
    "        date  = datetime.fromisoformat(row[\"date\"]).date()\n",
    "        if figi not in history:\n",
    "            if figi not in surprise:\n",
    "                surprise[figi] = { ticker : { 'first_time': date, 'unknown_figi': True } }\n",
    "            else:\n",
    "                surprise[figi][ticker]['last_time'] = date\n",
    "            continue\n",
    "        else:\n",
    "            history_figi = history[figi]\n",
    "            if ticker not in history_figi:\n",
    "                # known FIGI, but new ticker\n",
    "                if figi not in surprise:\n",
    "                    surprise[figi] = { ticker: { 'first_time': date, 'unknown_ticker': True } }\n",
    "                elif ticker not in surprise[figi]:\n",
    "                    surprise[figi][ticker] = { 'first_time': date, 'unknown_ticker': True }\n",
    "                else:\n",
    "                    surprise[figi][ticker]['last_time'] = date\n",
    "                continue\n",
    "            else:\n",
    "                history_figi_ticker = history_figi[ticker]\n",
    "                if 'end' not in history_figi_ticker or history_figi_ticker['end'] is None:\n",
    "                    history_figi_ticker['end'] = date\n",
    "                    history_figi_ticker['delisted_utc'] = row['delisted_utc']\n",
    "                else:\n",
    "                    history_figi_ticker['last_inactive'] = date\n",
    "surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ok. so there are several observations here:\n",
    "\n",
    "1) it happens that there appears a record about inactive ticker with figi while the there were no such figi among \n",
    "active tickers.\n",
    "\n",
    "i did search manually for BBG00FZLQV86, BBG000F4MYC2, BBG000GQR282 in the active tickers csv and haven't found anything.\n",
    "while they can be found in the inactive tickers csv.\n",
    "\n",
    "also this can be proven via https://polygon.io/docs/rest/stocks/tickers/all-tickers\n",
    "for example, for CNET on 2020-10-13 there is only one active ticker with \"last_updated_utc\": \"2016-05-18T00:00:00Z\".\n",
    "on 2020-10-14 there appears BBG00DP4PXQ7 with \"delisted_utc\": \"2020-10-14T00:00:00Z\". looks like there were 4 years \n",
    "without deals for this ticker and figi was not updated until delisting.\n",
    "\n",
    "2) if the figi is known for a ticker then the first deactivation record appears always after active records. means \n",
    "no crap here. but again - it's possible that there were no active records.\n",
    "\n",
    "3) it happens that for a given figi deactivation records stop to appear. a good example is BBG00GVX5D49 that appeared\n",
    "only once on 2021-08-02.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code below shows that it's possible that the ticker remains active even after an inactive record appears.\n",
    "count = 0\n",
    "for figi, tickers in history.items():\n",
    "    for ticker, dates in tickers.items():\n",
    "        last_active_date = dates.get('last_active', None)\n",
    "        end_date = dates.get('end', None)\n",
    "        if last_active_date and end_date and last_active_date > end_date:\n",
    "            # This is a case where the last active date is after the end date, which is unexpected\n",
    "            print(f\"Unexpected dates for {figi} {ticker}: last_active={last_active_date}, end={end_date}\")\n",
    "            count += 1\n",
    "    if count > 50:\n",
    "        print(\"found enough already\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more observations\n",
    "# ticker events api doesn't have data for delisted tickers. for example:\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# Add the parent directory to Python path to import api_key module\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "import api_key\n",
    "from polygon import RESTClient\n",
    "\n",
    "API_KEY = api_key.read_api_key()\n",
    "client = RESTClient(API_KEY)\n",
    "\n",
    "print(\"first check that api works for META\")\n",
    "events = client.get_ticker_events('META')\n",
    "print(events)\n",
    "\n",
    "print()\n",
    "print(\"now check that it doesn't return data for the delisted ticker FNM\")\n",
    "try:\n",
    "    events = client.get_ticker_events('FNM')\n",
    "    print(events)\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching events for FNM: {e}\")\n",
    "\n",
    "print()\n",
    "print(\"now check that there are ticker details for FNM in the past:\")\n",
    "details = client.get_ticker_details('FNM', '2004-11-01')\n",
    "print(details)\n",
    "\n",
    "print()\n",
    "print(\"and there are aggregates for FNM in the past:\")\n",
    "aggs = client.get_aggs('FNM', 1, 'day', '2004-11-01', '2004-11-02')\n",
    "print(aggs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but in fact from history analysis i can see:\n",
    "def print_details(ticker, date):\n",
    "    try:\n",
    "        details = client.get_ticker_details(ticker, date)\n",
    "        print(f'{ticker} on {date}: {details}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching details for {ticker} on {date}: {e}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def timestamp_to_date(timestamp):\n",
    "    try:\n",
    "        date = pd.to_datetime(timestamp, unit='ms').date()\n",
    "        return date\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting timestamp {timestamp} to date: {e}\")\n",
    "        return None\n",
    "\n",
    "def print_day_of_week(date):\n",
    "    try:\n",
    "        day_of_week = pd.to_datetime(date).day_name()\n",
    "        print(f'{date} is a {day_of_week}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching day of week for {date}: {e}\")\n",
    "\n",
    "def print_aggs(ticker, start_date, end_date):\n",
    "    try:\n",
    "        aggs = client.get_aggs(ticker, 1, 'day', start_date, end_date)\n",
    "        print(f'{ticker} from {start_date} to {end_date}: {aggs}')\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching aggregates for {ticker} from {start_date} to {end_date}: {e}\")\n",
    "\n",
    "def pretty_print_map(data):\n",
    "    for key, value in data.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "print(\"# ticker events API says that FORTY was renamed from FORT on 2004-11-08:\")\n",
    "print()\n",
    "# sometimes ticker events API is missleading.\n",
    "# for example, for FORTY it says that there was a ticker change\n",
    "events = client.get_ticker_events('FORTY')\n",
    "print(events.events[0])\n",
    "print(events.events[1])\n",
    "print()\n",
    "print(\"# that is wrong because it was trading under ticker FORTY since the beginning. see in aggregates API:\")\n",
    "print_aggs('FORTY', '2003-09-10', '2003-09-10')\n",
    "print(f'1063166400000 is {timestamp_to_date(1063166400000)}')\n",
    "\n",
    "print(\n",
    "'''\n",
    "# proof from flat files that FORTY was traded under ticker FORTY on 2003-09-10:\n",
    "ticker,conditions,correction,exchange,id,participant_timestamp,price,sequence_number,sip_timestamp      ,size,tape,trf_id,trf_timestamp\n",
    "\n",
    "FORTY,           ,0         ,11      ,  ,0                    ,12.13,54390          ,1063200772000000000,500 ,3   ,0,0\n",
    "FORTY,,0,12,,0,12.18,54683,1063200774000000000,500,3,0,0\n",
    "FORTY,,0,12,,0,12.02,164233,1063201566000000000,100,3,0,0\n",
    "FORTY,,0,12,,0,12,164235,1063201566000000000,300,3,0,0\n",
    "FORTY,,0,12,,0,12,186085,1063201737000000000,700,3,0,0\n",
    "FORTY,,0,3,,0,12.01,186176,1063201738000000000,300,3,0,0\n",
    "FORTY,,0,12,,0,12.02,405193,1063203807000000000,100,3,0,0\n",
    "FORTY,,0,12,,0,12.01,405213,1063203808000000000,200,3,0,0\n",
    "FORTY,,0,12,,0,12.011,451218,1063204318000000000,100,3,0,0\n",
    "FORTY,,0,12,,0,12.01,1255112,1063218714000000000,1800,3,0,0\n",
    "FORTY,,0,12,,0,12,1255113,1063218714000000000,200,3,0,0\n",
    "FORTY,,0,12,,0,12,1257062,1063218737000000000,1200,3,0,0\n",
    "FORTY,,0,3,,0,12,1257254,1063218739000000000,100,3,0,0\n",
    "FORTY,15,0,12,,0,12,1743433,1063224092000000000,0,3,0,0\n",
    "FORTY,15,0,3,,0,12,1746692,1063224185000000000,0,3,0,0\n",
    "''')\n",
    "\n",
    "print()\n",
    "print(\"# and it was never traded as FORT and FORT.Y ever:\")\n",
    "print_aggs('FORT', '2003-09-10', '2025-06-11')\n",
    "print_aggs('FORT.Y', '2003-09-10', '2025-06-11')\n",
    "\n",
    "print()\n",
    "print(\"# ticker details API also is wrong because it doesn't find FORTY before 2004-11-08:\")\n",
    "print_details('FORTY', '2004-11-01')\n",
    "print_day_of_week('2004-11-01')\n",
    "print()\n",
    "print(\"# while there were trades on 2004-11-01:\")\n",
    "print_aggs('FORTY', '2004-11-01', '2004-11-01')\n",
    "print()\n",
    "print(\"# the same on 2003-09-10:\")\n",
    "print_details('FORTY', '2003-09-10')\n",
    "print()\n",
    "print(\"# trades:\")\n",
    "print_aggs('FORTY', '2003-09-10', '2003-09-10')\n",
    "\n",
    "print()\n",
    "print(\"# history collected from active tickers also is not good because FORTY appears in it only in on 2004-11-08.\")\n",
    "print('history[\"BBG000JLM9R9\"] = {')\n",
    "pretty_print_map(history['BBG000JLM9R9'])\n",
    "print('}')\n",
    "print(\"# before 2004-11-08 there is no FORTY in tickers history. there is only FORT.Y for which there is no data in aggregates API.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "API looks unreliable. so for now my plan is to try to get the best possible data from what is available.\n",
    "\n",
    "the plan is to use the following algorithm:\n",
    "1. take the history from active tickers file.\n",
    "2. start from the first active date in the history.\n",
    "3. chose among all ticker for the same figi the one has has biggest USD volume.\n",
    "and add a check that if the ticker that was traded yesterday is also traded today and the price differs from the\n",
    "selected ticker more than 5% then log it as a potential issue.\n",
    "\n",
    "but the first step probably is to just dump all daily aggreages for all tickers in the history.\n",
    "this will speed up the process because i will not need to query the API for each ticker every time there is a doubt in\n",
    "which ticker to prefer.\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
