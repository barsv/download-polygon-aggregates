{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "code here downloads aggregates (bars) for a given ticker\n",
    "\n",
    "note: i created download_bars.py from this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "1. put api key into secret.txt file next to the download.ipynb\n",
    "2. change the list of tickers and set the timeframe (timespan and multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardcoded list of tickers\n",
    "TICKERS = ['SPY']  # Modify this list as needed\n",
    "\n",
    "timespan = 'minute' # second, minute, hour, day, week, month, quarter, year\n",
    "multiplier = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install polygon-api-client pandas python-dateutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "start the download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from polygon import RESTClient\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from dateutil import tz\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        # logging.FileHandler('polygon_download.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Read API key from secret.txt\n",
    "def read_api_key(file_path='secret.txt'):\n",
    "    \"\"\"Read API key from secret.txt.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            api_key = f.read().strip()\n",
    "        logger.info(\"Successfully read API key from secret.txt\")\n",
    "        return api_key\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading API key from {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Polygon API client\n",
    "API_KEY = read_api_key()\n",
    "if not API_KEY:\n",
    "    logger.error(\"No API key provided. Exiting.\")\n",
    "    sys.exit(1)\n",
    "client = RESTClient(API_KEY)\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = './stock_data/'  # Directory to save CSV files\n",
    "START_DATE = '2003-01-01'  # Start date for historical data\n",
    "END_DATE = datetime.now().strftime('%Y-%m-%d')  # Current date\n",
    "CHUNK_DAYS = 365  # Process one year at a time to manage memory\n",
    "RETRY_LIMIT = 3  # Number of retries for API failures\n",
    "RETRY_DELAY = 5  # Seconds to wait between retries\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def convert_to_et(timestamp_ms):\n",
    "    \"\"\"Convert Unix timestamp (ms) in UTC to Eastern Time (ET) datetime string.\"\"\"\n",
    "    utc_time = datetime.utcfromtimestamp(timestamp_ms / 1000).replace(tzinfo=tz.tzutc())\n",
    "    et_time = utc_time.astimezone(tz.gettz('America/New_York'))\n",
    "    return et_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def fetch_aggs(ticker, start_date, end_date):\n",
    "    \"\"\"Fetch second-by-second aggregates for a ticker within a date range.\"\"\"\n",
    "    data = []\n",
    "    attempt = 0\n",
    "    while attempt < RETRY_LIMIT:\n",
    "        try:\n",
    "            logger.info(f\"Fetching data for {ticker} from {start_date} to {end_date}\")\n",
    "            counter = 0\n",
    "            for agg in client.list_aggs(\n",
    "                ticker=ticker,\n",
    "                multiplier=multiplier,\n",
    "                timespan=timespan,\n",
    "                from_=start_date,\n",
    "                to=end_date,\n",
    "                adjusted=True,\n",
    "                sort='asc',\n",
    "                limit=50000\n",
    "            ):\n",
    "                data.append({\n",
    "                    'timestamp': convert_to_et(agg.timestamp),\n",
    "                    'open': agg.open,\n",
    "                    'high': agg.high,\n",
    "                    'low': agg.low,\n",
    "                    'close': agg.close,\n",
    "                    'volume': agg.volume,\n",
    "                    'vwap': agg.vwap,\n",
    "                    'transactions': agg.transactions,\n",
    "                    'otc': agg.otc\n",
    "                })\n",
    "                counter += 1\n",
    "                if counter % (50 * 1000) == 0:\n",
    "                    logger.info(f\"Retrieved {counter} records for {ticker}\")\n",
    "            logger.info(f\"Retrieved {len(data)} records for {ticker}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            logger.warning(f\"Attempt {attempt}/{RETRY_LIMIT} failed for {ticker}: {e}\")\n",
    "            if attempt < RETRY_LIMIT:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                logger.error(f\"Failed to fetch data for {ticker} after {RETRY_LIMIT} attempts\")\n",
    "                return []\n",
    "\n",
    "def save_to_csv(ticker, data):\n",
    "    \"\"\"Save data to a CSV file, appending if the file exists.\"\"\"\n",
    "    if not data:\n",
    "        logger.warning(f\"No data to save for {ticker}\")\n",
    "        return\n",
    "    df = pd.DataFrame(data)\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{ticker}.csv\")\n",
    "    logger.info(f\"Saving to {output_path}\")\n",
    "    try:\n",
    "        if os.path.exists(output_path):\n",
    "            # Append to existing file, avoid duplicating headers\n",
    "            df.to_csv(output_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            df.to_csv(output_path, mode='w', header=True, index=False)\n",
    "        logger.info(f\"Saved {len(df)} records to {output_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving CSV for {ticker}: {e}\")\n",
    "\n",
    "def process_ticker(ticker, start_date, end_date):\n",
    "    \"\"\"Process a single ticker, fetching data in chunks.\"\"\"\n",
    "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    current_start = start\n",
    "\n",
    "    while current_start < end:\n",
    "        current_end = min(current_start + timedelta(days=CHUNK_DAYS), end)\n",
    "        data = fetch_aggs(ticker, current_start.strftime('%Y-%m-%d'), current_end.strftime('%Y-%m-%d'))\n",
    "        save_to_csv(ticker, data)\n",
    "        current_start = current_end + timedelta(days=1)\n",
    "\n",
    "def main():\n",
    "    if not TICKERS:\n",
    "        logger.error(\"No tickers defined. Exiting.\")\n",
    "        return\n",
    "\n",
    "    for ticker in TICKERS:\n",
    "        logger.info(f\"Processing ticker: {ticker}\")\n",
    "        process_ticker(ticker, START_DATE, END_DATE)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
